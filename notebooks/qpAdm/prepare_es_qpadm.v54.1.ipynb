{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Files for qpAdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os  # For Saving to Folder\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import socket\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import itertools as it\n",
    "from time import time\n",
    "\n",
    "# For Arial Font\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'sans-serif'   # Set the defaul\n",
    "# Make sure to have the font installed (it is on cluster for Harald)\n",
    "rcParams['font.sans-serif'] = ['Arial']\n",
    "\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "\n",
    "if socket_name.startswith(\"compute-\"):\n",
    "    print(\"HSM Computational partition detected.\")\n",
    "    path = \"/n/groups/reich/hringbauer/git/punic_aDNA/\"  # The Path on Midway Cluster\n",
    "else:\n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "\n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "# Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(os.getcwd())\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_pops(df, string, col=\"clst\", output=1):\n",
    "    \"\"\"Return list of clusters that contain string.\"\"\"\n",
    "    df1 = df[df[col].str.contains(string)]\n",
    "    \n",
    "    if output==1:\n",
    "        print(f\"{string}: {len(df1)} \")\n",
    "    elif output==2:\n",
    "        print(df1[col].value_counts())\n",
    "        \n",
    "    clsts = list(set(df1[col].values))\n",
    "    return clsts\n",
    "\n",
    "def run_convertf(path_convertf = \"./o2bin/convertf\", parfile = \"./explore_ntbk/parfiles/convertf.keep.par\"):\n",
    "    \"\"\"Runs the Downsampling\"\"\"\n",
    "    ! $path_convertf -p $parfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/groups/reich/DAVID/V54/V54.1/v54.1_1240k_all\n"
     ]
    }
   ],
   "source": [
    "version = \"54.1\"\n",
    "v0 = version.split(\".\")[0]\n",
    "\n",
    "#base_path = f\"/n/groups/reich/DAVID/V{v0}/V{version}/v{version}\"\n",
    "base_path = f\"/n/groups/reich/DAVID/V{v0}/V{version}/v{version}_1240k_all\"\n",
    "#base_path = f\"/n/groups/reich/DAVID/V{v0}/V{version}/1240k/v{version}_1240k\"\n",
    "print(base_path)\n",
    "ind_path = base_path + \".ind\"\n",
    "\n",
    "pops = [\"Algeria\", \"Morocco\", \"Tunisia\", \"Punic\", \"Phoenician\", \"Spain_Vandal\", \"Spain_LBA\",\n",
    "        \"Sardinia\", \"Ibiza\",  \"Israel_MLBA\", \"Israel_LBA\", \"Israel_IA\", \"Israel_LIA\", \"Ashkelon\", \"Sicily\", \"Hellenistic\",\n",
    "        \"Israel_IA\", \"Israel_EIA\", \"Israel_Persian\", \"Gibraltar\", \"Lebanon\",\n",
    "        \"Spain_EBA_Afric\", \"Spain_BellBeaker_oAfrica\", \"Spain_Greek\",\n",
    "        \"Spain_Hellenistic\", \"Spain_IA\", \"Italy_Sardinia_N_oAfrica\", \n",
    "        \"Nigeria_IA\", \"Nigeria_Medieval\", \"Mallorca\", \"Menorca\", \n",
    "        \"Egypt_Hellenistic\", \"Egypt_Roman\", \"Egypt_Dynastic\", \"Egypt_Third\",\n",
    "        \"Spain_Roman_oAfrica2\", \"Formentera\", \"Aritgues\",  \"Greece_\", \"Guanche\", \"Israel_C\", \n",
    "        \"Spain_EN\", \"France_EN\"]\n",
    "\n",
    "exclude_strings = [\"contam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 33967 Individuals\n"
     ]
    }
   ],
   "source": [
    "df_ind = pd.read_csv(ind_path, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "print(f\"Loaded {len(df_ind)} Individuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spain: 1624 \n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "### Testing for single Populations Populations\n",
    "return_pops(df_ind, string=\"Spain\", \n",
    "            output=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include all relevant clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algeria: 6 \n",
      "Morocco: 18 \n",
      "Tunisia: 85 \n",
      "Punic: 194 \n",
      "Phoenician: 32 \n",
      "Spain_Vandal: 6 \n",
      "Spain_LBA: 60 \n",
      "Sardinia: 244 \n",
      "Ibiza: 1 \n",
      "Israel_MLBA: 60 \n",
      "Israel_LBA: 6 \n",
      "Israel_IA: 55 \n",
      "Israel_LIA: 1 \n",
      "Ashkelon: 10 \n",
      "Sicily: 272 \n",
      "Hellenistic: 102 \n",
      "Israel_IA: 55 \n",
      "Israel_EIA: 1 \n",
      "Israel_Persian: 1 \n",
      "Gibraltar: 4 \n",
      "Lebanon: 47 \n",
      "Spain_EBA_Afric: 3 \n",
      "Spain_BellBeaker_oAfrica: 2 \n",
      "Spain_Greek: 10 \n",
      "Spain_Hellenistic: 6 \n",
      "Spain_IA: 59 \n",
      "Italy_Sardinia_N_oAfrica: 2 \n",
      "Nigeria_IA: 4 \n",
      "Nigeria_Medieval: 3 \n",
      "Mallorca: 1 \n",
      "Menorca: 8 \n",
      "Egypt_Hellenistic: 6 \n",
      "Egypt_Roman: 4 \n",
      "Egypt_Dynastic: 5 \n",
      "Egypt_Third: 4 \n",
      "Spain_Roman_oAfrica2: 1 \n",
      "Formentera: 2 \n",
      "Aritgues: 8 \n",
      "Greece_: 139 \n",
      "Guanche: 5 \n",
      "Israel_C: 59 \n",
      "Spain_EN: 21 \n",
      "France_EN: 9 \n",
      "Loaded 458 Populations\n",
      "After Exclusion 438\n"
     ]
    }
   ],
   "source": [
    "clsts = [return_pops(df_ind, string=pop, output=1) for pop in pops]\n",
    "\n",
    "clsts = [inner for ls in clsts for inner in ls]\n",
    "clsts = list(set(clsts)) # Filter to unique Elements\n",
    "print(f\"Loaded {len(clsts)} Populations\")\n",
    "\n",
    "### Exclude Strings\n",
    "for ex in exclude_strings:\n",
    "    clsts = [c for c in clsts if ex not in c]\n",
    "print(f\"After Exclusion {len(clsts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add all populations from sample_list.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 184/188 not exclude\n",
      "184/184 Individuals found!\n",
      "Identified 74 Cluster to add\n",
      "3 clusters not already added.\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"./data/sample_list.v54.1.tsv\", sep=\"\\t\")\n",
    "dft = df1[df1[\"suggested Group ID (Ilan)\"]!=\"Exclude\"]\n",
    "print(f\"Filtered to {len(dft)}/{len(df1)} not exclude\")\n",
    "iids = dft[\"Version ID\"].values\n",
    "\n",
    "idx = df_ind[\"iid\"].isin(iids)\n",
    "print(f\"{np.sum(idx)}/{len(dft)} Individuals found!\")\n",
    "assert(np.sum(idx)==len(dft)) # To make sure all indivduals found\n",
    "### If assertion broken use this code to check\n",
    "#idx = np.array([iid in df_ind[\"iid\"].values for iid in iids])\n",
    "#iids[~idx]\n",
    "\n",
    "clsts_add = set(df_ind.loc[idx, \"clst\"])\n",
    "print(f\"Identified {len(clsts_add)} Cluster to add\")\n",
    "\n",
    "clsts_add1 = [clst for clst in clsts_add if clst not in clsts]\n",
    "print(f\"{len(clsts_add1)} clusters not already added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsts = clsts + clsts_add1 # Add cluster labels\n",
    "assert(len(set(clsts))==len(clsts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude Kerkoune for sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clsts = [c for c in clsts if \"Tunisia_Punic\" not in c]\n",
    "#print(f\"Filtered to n={len(clsts)} Cluster labels to include\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save List of populations to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 442 clusters to keep to: ./parfiles/keep_pops.v54.1\n"
     ]
    }
   ],
   "source": [
    "clsts = clsts + [\"include\"]\n",
    "#keep = np.array([\"Anatolia_N\", \"Iberia_HG\"])\n",
    "keep = np.array(clsts)\n",
    "path_keep = f\"./parfiles/keep_pops.v{version}\"\n",
    "np.savetxt(path_keep, keep, fmt=\"%s\")\n",
    "print(f\"Saved {len(clsts)} clusters to keep to: {path_keep}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flag out individuals who should not be extracted\n",
    "Idea: Some individuals should not be included in the final .ind file. To do this,\n",
    "I create a .ind file where the population of these is set to \"Ignore1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flagged out 892/33967 downsampled Individuals\n",
      "Saved to: /n/groups/reich/hringbauer/Data/v54.1.all.flagged.ind\n"
     ]
    }
   ],
   "source": [
    "save_path = f\"/n/groups/reich/hringbauer/Data/v{version}.all.flagged.ind\"\n",
    "\n",
    "ind_path = base_path + \".ind\"\n",
    "\n",
    "df_ind = pd.read_csv(ind_path, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "\n",
    "idx = df_ind[\"iid\"].str.endswith(\"_d\")\n",
    "df_ind.loc[idx, \"clst\"] = \"Ignore1\"\n",
    "print(f\"Flagged out {np.sum(idx)}/{len(idx)} downsampled Individuals\")\n",
    "df_ind.to_csv(save_path, header=False, sep=\" \", index=False)\n",
    "print(f\"Saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include Indivdiuals from Ilan's list.\n",
    "Include additional individuals from Ilans list.\n",
    "These include the classical outgroups.\n",
    "To do so set their clst to include (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Including 154/154 IIDs from: ./data/v54-added-samples.txt\n",
      "Saved modified .ind file to: /n/groups/reich/hringbauer/Data/v54.1.all.flagged.included.ind\n"
     ]
    }
   ],
   "source": [
    "save_path2 = f\"/n/groups/reich/hringbauer/Data/v{version}.all.flagged.included.ind\"\n",
    "path_external = \"./data/v54-added-samples.txt\"\n",
    "\n",
    "df_add = pd.read_csv(path_external, header=None, sep=r\"\\s+\", engine=\"python\")\n",
    "df_add.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "df_ind = pd.read_csv(save_path, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "\n",
    "### Add the additional Indivudals\n",
    "add_inds = [\"I13517_d\", \"I13518\", \"I13519_d\"] # Renamed indivdual plus some Myceneans\n",
    "search_inds = np.concatenate((df_add[\"iid\"], add_inds))\n",
    "\n",
    "idx = df_ind[\"iid\"].isin(search_inds)\n",
    "print(f\"Including {np.sum(idx)}/{len(search_inds)} IIDs from: {path_external}\")\n",
    "df_ind.loc[idx, \"clst\"] = \"include\"\n",
    "\n",
    "df_ind.to_csv(save_path2, header=False, sep=\" \", index=False)\n",
    "print(f\"Saved modified .ind file to: {save_path2}\")\n",
    "\n",
    "##################\n",
    "# Use codeblock for trouble shooting\n",
    "# idx = np.array([iid in df_ind[\"iid\"].values for iid in search_inds])\n",
    "# search_inds[~idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run convertf (with population list to keep)\n",
    "Additional parameters (such as position of output file) are coded into the parameter file\n",
    "\n",
    "Takes about 10-15 minutes for all individuals (v51.1)\n",
    "Takes minutes for 1506 indivdiuals (v54.1 dataset!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter file: ./parfiles/qpAdm/convertf.anc_only.54.1.par\n",
      "BASE: /n/groups/reich/\n",
      "DIR: DAVID/V54/V54.1/v54.1_1240k_all\n",
      "OUT: hringbauer/git/punic_aDNA/eigenstrat/anc_only.v54.1\n",
      "genotypename: /n/groups/reich//DAVID/V54/V54.1/v54.1_1240k_all.geno\n",
      "snpname: /n/groups/reich//DAVID/V54/V54.1/v54.1_1240k_all.snp\n",
      "indivname: /n/groups/reich/hringbauer/Data/v54.1.all.flagged.included.ind\n",
      "genooutfilename: /n/groups/reich//hringbauer/git/punic_aDNA/eigenstrat/anc_only.v54.1.geno\n",
      "snpoutfilename: /n/groups/reich//hringbauer/git/punic_aDNA/eigenstrat/anc_only.v54.1.snp\n",
      "indoutfilename: /n/groups/reich//hringbauer/git/punic_aDNA/eigenstrat/anc_only.v54.1.ind\n",
      "outputformat: PACKEDANCESTRYMAP\n",
      "hashcheck: YES\n",
      "poplistname: /n/groups/reich//hringbauer/git/punic_aDNA/parfiles/keep_pops.v54.1\n",
      "## /n/groups/reich/hringbauer/o2bin/convertf version: 8150\n",
      "read 1073741824 bytes\n",
      "read 2147483648 bytes\n",
      "read 3221225472 bytes\n",
      "read 4294967296 bytes\n",
      "read 5368709120 bytes\n",
      "read 6442450944 bytes\n",
      "read 7516192768 bytes\n",
      "read 8589934592 bytes\n",
      "read 9663676416 bytes\n",
      "read 10470746396 bytes\n",
      "packed geno read OK\n",
      "end of inpack\n",
      "setting HG03742.DG to hasxhets = NO (male!)\n",
      "setting HG03875.DG to hasxhets = NO (male!)\n",
      "setting NA19236.DG to hasxhets = NO (male!)\n",
      "setting HG02554.DG to hasxhets = NO (male!)\n",
      "setting NA12814.DG to hasxhets = NO (male!)\n",
      "before compress: snps: 1233013 indivs: 33967\n",
      "after compress: snps: 1233013 indivs: 1506\n",
      "numvalidind:   1506  maxmiss: 1506001\n",
      "callng outfiles\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_convertf(path_convertf = \"/n/groups/reich/hringbauer/o2bin/convertf\", \n",
    "             parfile = f\"./parfiles/qpAdm/convertf.anc_only.{version}.par\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Merge in Lazaridis Ancients with mergeit\n",
    "### ATTENTION Not needed anymore as of 49.2 as samples are merged in automatically\n",
    "\n",
    " See older version of this code if want to revive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1506 Individuals in Analysis .ind\n",
      "Found 154 Indivduals that have been manually added\n"
     ]
    }
   ],
   "source": [
    "### Sanity Check\n",
    "path_original = f\"./eigenstrat/anc_only.v{version}.ind\" \n",
    "df_ind = pd.read_csv(path_original, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "print(f\"Loaded {len(df_ind)} Individuals in Analysis .ind\")\n",
    "\n",
    "idx = df_ind[\"clst\"] == \"include\"\n",
    "print(f\"Found {np.sum(idx)} Indivduals that have been manually added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset Cluster Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 33967 original individuals\n",
      "Loaded 1506 Individuals in newly generated .ind\n",
      "Saved original 1506 IIDs to: ./eigenstrat/anc_only.v54.1.org.ind\n",
      "Found 154 Indivduals that have been manually added\n",
      "Replace df of length 154 created\n",
      "Saved updated 1506 IIDs to: ./eigenstrat/anc_only.v54.1.ind\n"
     ]
    }
   ],
   "source": [
    "### Load Original Individuals\n",
    "path_load = f\"/n/groups/reich/DAVID/V{v0}/V{version}/v{version}_1240k_all.ind\"\n",
    "#ind_merged = f\"./eigenstrat/combined/punic.v{version}.ind\"          # What .ind to load\n",
    "#path_save_original = f\"./eigenstrat/combined/punic.v{version}.org.ind\"\n",
    "path_original = f\"./eigenstrat/anc_only.v{version}.ind\" \n",
    "path_save_original = f\"./eigenstrat/anc_only.v{version}.org.ind\"\n",
    "\n",
    "df_org = pd.read_csv(path_load, delim_whitespace=True, header=None)\n",
    "df_org.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "print(f\"Loaded {len(df_org)} original individuals\")\n",
    "\n",
    "### Load the Newly generated .ind\n",
    "df_ind = pd.read_csv(path_original, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "print(f\"Loaded {len(df_ind)} Individuals in newly generated .ind\")\n",
    "df_ind.to_csv(path_save_original, header=False, sep=\" \", index=False)\n",
    "print(f\"Saved original {len(df_ind)} IIDs to: {path_save_original}\")\n",
    "\n",
    "idx = df_ind[\"clst\"] == \"include\"\n",
    "print(f\"Found {np.sum(idx)} Indivduals that have been manually added\")\n",
    "iids = df_ind.loc[idx, \"iid\"].values\n",
    "\n",
    "### Filter to Indivdiuals to replace\n",
    "df_replace = df_org[df_org[\"iid\"].isin(iids)].copy().reset_index(drop=True)\n",
    "print(f\"Replace df of length {len(df_replace)} created\")\n",
    "\n",
    "assert(len(iids)==len(df_replace)) # Sanity Check\n",
    "df_ind.set_index('iid', inplace=True, drop=False)\n",
    "df_replace.set_index('iid', inplace=True, drop=False)\n",
    "df_ind.update(df_replace, overwrite=True) ## Update in place\n",
    "assert(np.sum(df_ind.isnull().values)==0) # Sanity Check\n",
    "\n",
    "### Final saving. Overwrite original file\n",
    "df_ind.to_csv(path_original, header=False, sep=\" \", index=False)\n",
    "print(f\"Saved updated {len(df_ind)} IIDs to: {path_original}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Individual File [Stand Alone from here]\n",
    "Overwrite Individuals with their individual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overwrite_ind_df(df, string, col=\"clst\", \n",
    "                     output=False, overwrite=\"\", iids=False):\n",
    "    \"\"\"Overwrite Individual Dataframe where col\n",
    "    contains string. Return modified dataframe (Copy)\n",
    "    where overwrite is the new Cluster ID\n",
    "    iids: Overwrite with IIDs if True!\"\"\"\n",
    "    idx = df[col].str.contains(string)\n",
    "    \n",
    "    if np.sum(idx)==0:\n",
    "        if output: \n",
    "            print(\"No Indivdiuals found\")\n",
    "        return\n",
    "    \n",
    "    if output:\n",
    "        print(f\"Found {np.sum(idx)} Matches\")\n",
    "        print(df[idx][col].value_counts())\n",
    "    \n",
    "    ### Actually  overwrite the Column\n",
    "    if len(overwrite)>0:\n",
    "        df.loc[idx, col] = overwrite\n",
    "        if output: \n",
    "            print(f\"{np.sum(idx)} Overwritten!\")\n",
    "            \n",
    "    if iids:\n",
    "        df.loc[idx, col] = df.loc[idx, \"iid\"] \n",
    "        \n",
    "        \n",
    "### Overwrite Individual IIDs\n",
    "def modifiy_iid_files(df_ind, pops_overwrite, \n",
    "                      pops_overwrite12=[], ind_modified=\"\"):\n",
    "    \"\"\"Modify .ind file. Overwrite individuals from pops_overwrite (list)\n",
    "    with their individuals labels. \n",
    "    df_int: Dataframe from Individuals.\n",
    "    pops_overwrite12: [[pop1,pop2]] list (nx2). Overwrites ALL string matches\n",
    "    for pop1 (contain) with pop2\"\"\"\n",
    "    \n",
    "    ### Overwrite with other Label\n",
    "    for pop1,pop2 in pops_overwrite12:\n",
    "        overwrite_ind_df(df_ind, pop1, overwrite=pop2)\n",
    "        \n",
    "    ### Overwrite with individual IIds\n",
    "    for pop in pops_overwrite:\n",
    "        overwrite_ind_df(df_ind, pop, \n",
    "                     iids=True, output=True)\n",
    "    \n",
    "    ### Save here\n",
    "    df_ind.to_csv(ind_modified, sep=\" \", index=None, header=False)\n",
    "    print(f\"Saved {len(df_ind)} Individuals to {ind_modified}\")\n",
    "    \n",
    "def set_clst_to_iid(df_ind, iids_overwrite, \n",
    "                    pops_overwrite12=[], savepath=\"\"):\n",
    "    \"\"\"Modify .ind file. Overwrite individuals from pops_overwrite (list)\n",
    "    with their individuals labels. \n",
    "    df_int: Dataframe from Individuals.\n",
    "    pops_overwrite12: [[pop1,pop2]] list (nx2). Overwrites ALL string matches\n",
    "    for pop1 (contain) with pop2\"\"\"\n",
    "    \n",
    "    ### Overwrite with other Label\n",
    "    idx = df_ind[\"iid\"].isin(iids)\n",
    "    print(f\"Overwriting {np.sum(idx)} Individuals\")\n",
    "    df_ind.loc[idx, \"clst\"] = df_ind.loc[idx, \"iid\"]\n",
    "        \n",
    "    ### Overwrite with other Label\n",
    "    for pop1,pop2 in pops_overwrite12:\n",
    "        overwrite_ind_df(df_ind, pop1, overwrite=pop2)\n",
    "    \n",
    "    ### Save here\n",
    "    if len(savepath)>0:\n",
    "        df_ind.to_csv(savepath, sep=\" \", index=None, header=False)\n",
    "        print(f\"Saved {len(df_ind)} Individuals to {savepath}\")\n",
    "        \n",
    "def set_iids_to_clst(df_ind, iids=[], clst=\"\", savepath=\"\"):\n",
    "    \"\"\"Set List of Indivdiuals to Cluster Label.\n",
    "    savepath: If defined: Save to there.\"\"\"\n",
    " \n",
    "    idx = df_ind[\"iid\"].isin(iids)\n",
    "    print(f\"Overwriting {np.sum(idx)} Individuals to {clst}\")\n",
    "    df_ind.loc[idx, \"clst\"] = clst\n",
    "    \n",
    "    ### Save here\n",
    "    if len(savepath)>0:\n",
    "        df_ind.to_csv(savepath, sep=\" \", index=None, header=False)\n",
    "        print(f\"Saved {len(df_ind)} Individuals to {savepath}\")  \n",
    "    return df_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Set clusters from Lazaridis\n",
    "Update since v49.2: Reset the clusters from Lazaridis et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1506 original individuals\n",
      "Loaded 353 label individuals\n",
      "Mota: 1\n",
      "Overwriting 1 Individuals to Mota\n",
      "Ust_Ishim: 1\n",
      "Overwriting 1 Individuals to Ust_Ishim\n",
      "Kostenki14: 1\n",
      "Overwriting 1 Individuals to Kostenki14\n",
      "GoyetQ116-1: 1\n",
      "Overwriting 1 Individuals to GoyetQ116-1\n",
      "Vestonice16: 1\n",
      "Overwriting 1 Individuals to Vestonice16\n",
      "MA1: 1\n",
      "Overwriting 1 Individuals to MA1\n",
      "ElMiron: 1\n",
      "Overwriting 1 Individuals to ElMiron\n",
      "Villabruna: 1\n",
      "Overwriting 1 Individuals to Villabruna\n",
      "EHG: 3\n",
      "Overwriting 3 Individuals to EHG\n",
      "CHG: 2\n",
      "Overwriting 2 Individuals to CHG\n",
      "Natufian: 6\n",
      "Overwriting 6 Individuals to Natufian\n",
      "Levant_N: 13\n",
      "Overwriting 13 Individuals to Levant_N\n",
      "Anatolia_N: 26\n",
      "Overwriting 26 Individuals to Anatolia_N\n",
      "WHG: 3\n",
      "Overwriting 3 Individuals to WHG\n",
      "Steppe_EMBA: 26\n",
      "Overwriting 26 Individuals to Steppe_EMBA\n",
      "Iran_N: 9\n",
      "Overwriting 9 Individuals to Iran_N\n",
      "Saved updated 1506 IIDs to: ./eigenstrat/anc_only.v54.1_outgroups.ind\n"
     ]
    }
   ],
   "source": [
    "path_target = f\"./eigenstrat/anc_only.v{version}.ind\"          # What .ind to load\n",
    "path_save = f\"./eigenstrat/anc_only.v{version}_outgroups.ind\" \n",
    "path_labels =  f\"/n/groups/reich/hringbauer/git/punic_aDNA/eigenstrat/additional/MinMyc.v{version}.ind\"\n",
    "\n",
    "groups = [\"Mota\", \"Ust_Ishim\", \"Kostenki14\", \n",
    "       \"GoyetQ116-1\", \"Vestonice16\", \"MA1\",\n",
    "       \"ElMiron\", \"Villabruna\", \"EHG\", \"CHG\", \"Natufian\",\n",
    "       \"Levant_N\", \"Anatolia_N\", \"WHG\", \"Steppe_EMBA\",\n",
    "       \"Iran_N\"] # \"Morocco_EN\" not in Myceneans\n",
    "\n",
    "### Load the Individuals\n",
    "df_ind = pd.read_csv(path_target, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "print(f\"Loaded {len(df_ind)} original individuals\")\n",
    "\n",
    "df_lbls = pd.read_csv(path_labels, delim_whitespace=True, header=None)\n",
    "df_lbls.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "print(f\"Loaded {len(df_lbls)} label individuals\")\n",
    "\n",
    "### Reset Clusters\n",
    "for g in groups:\n",
    "    idx = (df_lbls[\"clst\"] == g)\n",
    "    print(f\"{g}: {np.sum(idx)}\")\n",
    "    iids = df_lbls[\"iid\"][idx].values\n",
    "    \n",
    "    ### Reset All the Individuals in the Target Ind\n",
    "    df_ind = set_iids_to_clst(df_ind, iids=iids, clst=g, savepath=\"\")\n",
    "    \n",
    "df_ind.to_csv(path_save, header=False, sep=\" \", index=False)\n",
    "print(f\"Saved updated {len(df_ind)} IIDs to: {path_save}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Only overwrite the Punics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 190 Matches\n",
      "Tunisia_Punic                                            36\n",
      "Italy_Sicily_Punic_Possible                              18\n",
      "Italy_Sicily_Punic_Early                                 15\n",
      "Tunisia_Punic_Africa                                      7\n",
      "Italy_Sardinia_Punic_Early                                6\n",
      "Italy_Sicily_Punic_Roman                                  6\n",
      "Spain_Punic                                               6\n",
      "Spain_Punic_Late                                          6\n",
      "Italy_Sicily_Punic                                        5\n",
      "Italy_Sicily_Punic_Late                                   5\n",
      "Tunisia_Punic.SG                                          5\n",
      "Italy_Sardinia_IA_Punic_1                                 5\n",
      "Tunisia_Punic_oAfrica2.SG                                 4\n",
      "Italy_Sardinia_IA_Punic_2                                 4\n",
      "Tunisia_Punic_lc                                          4\n",
      "Italy_Sardinia_Punic_Late_oNAfrica                        3\n",
      "Italy_Sicily_Punic_Possible_lc                            3\n",
      "Tunisia_Punic_oAfricaHigh                                 3\n",
      "Spain_Punic_lc                                            3\n",
      "Tunisia_Punic_Phoenician                                  3\n",
      "Italy_Sardinia_Punic_Roman                                2\n",
      "Spain_Punic_oLNBA                                         2\n",
      "Spain_Punic_Late_oAfrica                                  2\n",
      "Spain_Punic_Late_lc                                       2\n",
      "Spain_Punic_o.3rd.degree.relative.cluster                 2\n",
      "Spain_Punic_lc_1d.rel.I27611_d                            1\n",
      "Spain_Punic_possible                                      1\n",
      "Italy_Sardinia_Punic_Roman_oNAfrica                       1\n",
      "Spain_Punic_Roman_oAfrica1                                1\n",
      "Spain_Punic_Roman_oAfrica2_lc                             1\n",
      "Austria_N_LBK_oAfrica_Tunisa_Punic_possible_mix_up_lc     1\n",
      "Spain_Punic_o1                                            1\n",
      "Spain_Punic_Early_oEurope                                 1\n",
      "Tunisia_Punic_1d.rel.I24199                               1\n",
      "Spain_Punic_Roman_oEuropean2                              1\n",
      "Spain_Punic_Roman                                         1\n",
      "Italy_Sicily_Punic_lc                                     1\n",
      "Tunisia_Punic_mother.or.daughter.I24040                   1\n",
      "Spain_Punic2_lc                                           1\n",
      "Tunisia_Punic_oAfrica                                     1\n",
      "Italy_Sardinia_Punic_lc                                   1\n",
      "Spain_Punic_African_lc                                    1\n",
      "Italy_Sardinia_Punic_Late                                 1\n",
      "Spain_Punic_Late_1d.rel.I26932                            1\n",
      "Spain_Punic_Roman_oAfrica3                                1\n",
      "Spain_Punic_o2                                            1\n",
      "Italy_Sardinia_Punic_Late_oLevant                         1\n",
      "Italy_Sardinia_Punic_Late_oAfrica                         1\n",
      "Spain_Punic1_contam_lc                                    1\n",
      "Ibiza_Punic.SG                                            1\n",
      "Tunisia_Punic_oAfrica1.SG                                 1\n",
      "Italy_Sardinia_Punic                                      1\n",
      "Italy_Sardinia_Punic_Late_oEurope                         1\n",
      "Spain_Punic_o.3rd.degree.relative.cluster_alt             1\n",
      "Spain_Punic_Roman_o3                                      1\n",
      "Tunisia_Punic_oAfrican                                    1\n",
      "Spain_Punic_Late_oEurope                                  1\n",
      "Italy_Sardinia_Punic_Early_lc                             1\n",
      "Name: clst, dtype: int64\n",
      "No Indivdiuals found\n",
      "Found 27 Matches\n",
      "Israel_Phoenician                           13\n",
      "Lebanon_Chhim_Phoenician.SG                  4\n",
      "Israel_Phoenician_lc                         3\n",
      "Lebanon_EjJaouze_Phoenician.SG               3\n",
      "Italy_Sardinia_SantImbenia_Phoenician.SG     2\n",
      "Israel_Phoenician_oUpperNile                 1\n",
      "Italy_Phoenician_Sicily                      1\n",
      "Name: clst, dtype: int64\n",
      "Saved 1506 Individuals to ./eigenstrat/combined/punic.v54.1_ind.ind\n"
     ]
    }
   ],
   "source": [
    "### Populations to overwrite. Typically because they have the \".SG\" label\n",
    "pops_overwrite12 = [[\"Morocco_LN\", \"Morocco_LN\"], [\"Morocco_EN\", \"Morocco_EN\"],\n",
    "                   [\"Morocco_Iberomaurusian\",\"Morocco_Iberomaurusian\"], [\"YRI\", \"YRI\"]]\n",
    "\n",
    "### Population to overwrite because they are the target\n",
    "pops_overwrite = [\"Punic\", \"Sicily_Punic\", \"Phoen\"] # \"Sardinia\", \"Spain\" \"Algeria\", \n",
    "\n",
    "ind_modified=f\"./eigenstrat/combined/punic.v{version}_ind.ind\"    # Where to save the modified version to\n",
    "\n",
    "modifiy_iid_files(df_ind, pops_overwrite=pops_overwrite, \n",
    "                  pops_overwrite12=pops_overwrite12,\n",
    "                  ind_modified = ind_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Split up Iberian Bronze Age and Sardinian Nuragic too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 Matches\n",
      "Algeria_NumidoRoman_Berber.SG    2\n",
      "Algeria_N                        1\n",
      "Name: clst, dtype: int64\n",
      "No Indivdiuals found\n",
      "No Indivdiuals found\n",
      "Found 19 Matches\n",
      "Italy_Sicily_IA_Polizzello    19\n",
      "Name: clst, dtype: int64\n",
      "Found 10 Matches\n",
      "Italy_Sicily_IA_Sicani               4\n",
      "Italy_Sicily_IA_Sicani_lc            4\n",
      "Italy_Sicily_IA_Sicani_Hellenized    2\n",
      "Name: clst, dtype: int64\n",
      "No Indivdiuals found\n",
      "Found 3 Matches\n",
      "Morocco_LN    3\n",
      "Name: clst, dtype: int64\n",
      "No Indivdiuals found\n",
      "No Indivdiuals found\n",
      "No Indivdiuals found\n",
      "No Indivdiuals found\n",
      "No Indivdiuals found\n",
      "No Indivdiuals found\n",
      "No Indivdiuals found\n",
      "No Indivdiuals found\n",
      "No Indivdiuals found\n",
      "No Indivdiuals found\n",
      "No Indivdiuals found\n",
      "Found 1 Matches\n",
      "Italy_Sardinia_C_o    1\n",
      "Name: clst, dtype: int64\n",
      "Found 18 Matches\n",
      "Italy_Sardinia_BA_Nuragic                                 10\n",
      "Italy_Sardinia_BA_Nuragic_o                                5\n",
      "Italy_Sardinia_EBA_Nuragic_mother.SUC003                   1\n",
      "Italy_Sardinia_EBA_Nuragic                                 1\n",
      "Italy_Sardinia_EBA_Nuragic_mother.SUC003_dup.SUC002.SG     1\n",
      "Name: clst, dtype: int64\n",
      "Found 4 Matches\n",
      "Nigeria_IA    4\n",
      "Name: clst, dtype: int64\n",
      "Found 3 Matches\n",
      "Nigeria_Medieval_lc    2\n",
      "Nigeria_Medieval       1\n",
      "Name: clst, dtype: int64\n",
      "Saved 1506 Individuals to ./eigenstrat/combined/punic.v54.1_ind1.ind\n"
     ]
    }
   ],
   "source": [
    "pops_overwrite = [\"Algeria_N\", \"Punic\", \"Italy_Sicily_Phoenician\",\n",
    "                  \"Sicily_IA_Polizzello\", \"Sicani\", \"Phoen\",\n",
    "                  \"Morocco_LN\", \"Punic_oAfrican\", \n",
    "                  \"Iberia_North_BA_Africa_all\", \"Iberia_BA\", \"Iberia_IA\",\n",
    "                  \"Iberia_Greek\", \"Iberia_Hellenistic\",\n",
    "                  \"Iberia_BellBeaker_o\", \"Gibraltar_N\", \n",
    "                  \"Iberia_Iberian\", \"Iberia_Celtiberian\", \"Iberia_Tartessian\",\n",
    "                  \"Italy_Sardinia_C_o\", \"Nuragic\",\n",
    "                  \"Nigeria_IA\", \"Nigeria_Medieval\"\n",
    "                  ]\n",
    "\n",
    "pops_overwrite12 = [[\"Morocco_LN\", \"Morocco_LN\"], [\"Morocco_EN\", \"Morocco_EN\"],\n",
    "                   [\"Morocco_Iberomaurusian\",\"Morocco_Iberomaurusian\"], [\"YRI\",\"YRI\"]]\n",
    "\n",
    "ind_modified=f\"./eigenstrat/combined/punic.v{version}_ind1.ind\"    # Where to save the modified version to\n",
    "\n",
    "modifiy_iid_files(df_ind, pops_overwrite=pops_overwrite, \n",
    "                  pops_overwrite12=pops_overwrite12,\n",
    "                  ind_modified = ind_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Modify all targets for distal modelling (Punics and Proximal Sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_merged=f\"./eigenstrat/combined/punic.v{version}.ind\"          # What .ind to load\n",
    "df_ind = pd.read_csv(ind_merged, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "print(f\"Loaded {len(df_ind)} Individuals\")\n",
    "\n",
    "df_meta = pd.read_csv(\"/n/groups/reich/hringbauer/Data/v49.0.anno.csv\")\n",
    "dft = pd.merge(df_ind[\"iid\"], df_meta[[\"iid\", \"loc\", \"clst\", \"n_cov_snp\", \"age\", \"lat\", \"lon\"]], on=\"iid\")\n",
    "\n",
    "df_labels = pd.read_csv(\"./data/qpAdm_pops.tsv\", sep=\"\\t\")\n",
    "\n",
    "dfs = []\n",
    "for index, row in df_labels.iterrows():\n",
    "    dft1 = dft[dft[\"clst\"].str.contains(row[\"clst\"]) & (dft[\"loc\"] == row[\"loc\"])]\n",
    "    dfs.append(dft1)\n",
    "    \n",
    "df_targets = pd.concat(dfs)\n",
    "\n",
    "print(f\"Saving meta of {len(df_targets)} Individuals for further processing.\")\n",
    "#df_targets.to_csv(\"./output/tables/qpadm.targets.distal.v46.3.tsv\", sep=\"\\t\", index=False)\n",
    "iids = df_targets[\"iid\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only save if additional Individuals need a rerun\n",
    "df_temp = pd.read_csv(\"./output/tables/qpadm.targets.distal.v46.3.tsv\", sep=\"\\t\")\n",
    "idx = [iid not in df_temp[\"iid\"].values for iid in iids]\n",
    "#df_targets[idx].to_csv(\"./output/tables/qpadm.targets.distal.v46.3.add.tsv\", sep=\"\\t\")\n",
    "print(f\"Saved {np.sum(idx)} Indivdiuals for a rerun.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_clst_to_iid(df_ind, iids, pops_overwrite12=[], \n",
    "                savepath=\"./eigenstrat/combined/punic.v46.3_ind2.ind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Modify Individual PCA clusters of Punic Individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1321 Individuals\n",
      "Overwriting 9 Individuals to PunicCline\n",
      "Overwriting 3 Individuals to PunicAfrican\n",
      "Saved 1321 Individuals to ./eigenstrat/combined/punic.v49.2_punic_clst.ind\n"
     ]
    }
   ],
   "source": [
    "ind_merged=f\"./eigenstrat/combined/punic.v{version}.ind\"          # What .ind to load\n",
    "df_ind = pd.read_csv(ind_merged, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "print(f\"Loaded {len(df_ind)} Individuals\")\n",
    "\n",
    "### Modify the Clusters\n",
    "iids_afr_punic =  [\"I18193\", \"I18189\", \"I22093\"]  # \"I22113\" high but not high enough\n",
    "iids_afr_cline = [\"I21966\", \"I21858\", \"I21857\", \"I7454\", \"VIL011\", \"VIL006\", \"VIL009\", \"VIL010\", \"VIL007\"] # but not VIL004\n",
    "\n",
    "df_ind = set_iids_to_clst(df_ind, iids=iids_afr_cline, clst=\"PunicCline\", savepath=\"\")\n",
    "df_ind = set_iids_to_clst(df_ind, iids=iids_afr_punic, clst=\"PunicAfrican\", savepath=\"\")\n",
    "\n",
    "savepath=f\"./eigenstrat/combined/punic.v{version}_punic_clst.ind\"\n",
    "df_ind.to_csv(savepath, sep=\" \", index=None, header=False)\n",
    "print(f\"Saved {len(df_ind)} Individuals to {savepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Share eigenstrat with Ilan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1506 Individuals\n"
     ]
    }
   ],
   "source": [
    "path_load = f\"./eigenstrat/anc_only.v{version}.ind\"  \n",
    "df_ind = pd.read_csv(path_load, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "print(f\"Loaded {len(df_ind)} Individuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 184/188 not exclude\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"./data/sample_list.tsv\", sep=\"\\t\")\n",
    "df2 = df1[df1[\"suggested Group ID (Ilan)\"]!=\"Exclude\"]\n",
    "print(f\"Filtered to {len(df2)}/{len(df1)} not exclude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 174/184\n"
     ]
    }
   ],
   "source": [
    "idx = np.array([iid in df_ind[\"iid\"].values for iid in  df2[\"Version ID\"].values])\n",
    "print(f\"Found {np.sum(idx)}/{len(idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>sex</th>\n",
       "      <th>clst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>I11896</td>\n",
       "      <td>F</td>\n",
       "      <td>Algeria_N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       iid sex       clst\n",
       "32  I11896   F  Algeria_N"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ind[df_ind[\"iid\"].str.contains(\"I11896\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df[\"iid\"]==\"I7267\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1506/1506 Samples\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/n/groups/reich/hringbauer/Data/v54.1.anno.csv\", sep=\",\")\n",
    "idx = df[\"iid\"].isin(df_ind[\"iid\"].values) | df[\"iid\"].isin([\"I7267_v54.1_addback\"])\n",
    "print(f\"Found {np.sum(idx)}/{len(df_ind)} Samples\")\n",
    "\n",
    "dft = df[idx]\n",
    "dft.to_csv(f\"./output/share/gronau.v{version}/meta.tsv\", sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1506"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>sex</th>\n",
       "      <th>clst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>I7267 _v54.1_addback</td>\n",
       "      <td>F</td>\n",
       "      <td>Italy_Sicily_Punic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       iid sex                clst\n",
       "1504  I7267 _v54.1_addback   F  Italy_Sicily_Punic"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx1 = np.array([iid in df[\"iid\"].values for iid in df_ind[\"iid\"]])\n",
    "df_ind[~idx1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the additional individuals to final eigenstrat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 144 additional Indivdiuals\n",
      "Saved modified 143 Table to: ./output/share/gronau.v49.2.add.ind.tsv\n"
     ]
    }
   ],
   "source": [
    "path_load = f\"./eigenstrat/anc_only.v{version}.ind\"   \n",
    "df_ind = pd.read_csv(path_load, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "\n",
    "df_add = pd.read_csv(\"./data/v49-added-samples.txt\", header=None, sep=r\"\\s+\", engine=\"python\")\n",
    "df_add.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "print(f\"Loaded {len(df_add)} additional Indivdiuals\")\n",
    "\n",
    "df2 = pd.merge(df_ind[[\"iid\", \"clst\"]], df_add[[\"iid\", \"clst\"]], \n",
    "               on=\"iid\", suffixes=('_new', '_add'))\n",
    "\n",
    "### Save the table for manual review\n",
    "save_path = \"./output/share/gronau.v49.2.add.ind.tsv\"\n",
    "df2.to_csv(save_path, sep=\"\\t\", index=False)\n",
    "print(f\"Saved modified {len(df2)} Table to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test loading single Eigenstrat File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_load = f\"/n/groups/reich/DAVID/V{v0}/V{version}/v{version}.ind\"\n",
    "df_ind = pd.read_csv(path_load, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ind[df_ind[\"clst\"].str.contains(\"Phoen\")]\n",
    "\n",
    "#pops = [\"Spain_IA_Tartessian\", \"Spain_IA_Celt\", \"Italy_Sardinia_BA_Nuragic\", \n",
    "#        \"Italy_Sicily_IA_Sicani\", \"Greece_BA_Mycenaean\", \"Israel_Phoenician\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to David's Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.read_csv(\"/n/groups/reich/hringbauer/Data/Unpublished_data.tsv\", sep=\"\\t\")\n",
    "age_col = \"Average of 95.4% date range in calBP (defined as 1950 CE)  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.str.contains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_col = 'Group_ID (format convention which we try to adhere to is \"Country_<Geographic.Region_<Geographic.Subregion_>><Archaeological.Period.Or.DateBP_<Alternative.Archaeological.Period_>><Archaeological.Culture_<Alternative.Archaeological.Culture>><genetic.subgrouping.index.if.necessary_><\"o_\"sometimes.with.additional.detail.if.an.outlier><additional.suffix.especially.relative.status.if.we.recommend.removing.from.main.analysis.grouping><\"contam_\".if.contaminated><\"lc_\".if.<15000.SNPs.on.autosomal.targets><\".SG\".or.\".DG\".if.shotgun.data>; HG=hunter-gatherer, N=Neolithic, C=Chalcolithic/CopperAge, BA=BronzeAge, IA=IronAge, E=Early, M=Middle, L=Late, A=Antiquity)'\n",
    "groups = df_t[group_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsts_list = \"|\".join(clsts)\n",
    "idx = groups.str.contains(clsts_list)\n",
    "print(f\"Found {np.sum(idx)} Individuals\")\n",
    "df_found = df_t[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_found[150:174][[\"Master ID\", group_col, \"Publication\", \"Locality\", age_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fto_csvo_csv(\"./output/tables/samples_claim.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Entries in Eigenstrat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1320 Individuals\n"
     ]
    }
   ],
   "source": [
    "#ind_merged=\"./eigenstrat/anc_only.v46.3.ind\"          # What .ind to load\n",
    "ind_merged=\"./eigenstrat/combined/punic.v49.2.ind\"          # What .ind to load\n",
    "df_ind = pd.read_csv(ind_merged, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "print(f\"Loaded {len(df_ind)} Individuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "include    147\n",
       "Name: clst, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ind[df_ind[\"clst\"].str.contains(\"include\")][\"clst\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Outgroup: \n",
    "### Israel_MLBA Italy_Sardinia_EBA Spain_LBA Italy_Sicily_EBA Steppe_MLBA Greece_Minoan_Lassithi\n",
    "### Additional Source: Spain_IA I12433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ind[\"clst\"].value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ind[df_ind[\"clst\"].str.contains(\"Algeria\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
