{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Files for qpAdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute-e-16-229.o2.rc.hms.harvard.edu\n",
      "HSM Computational partition detected.\n",
      "/n/groups/reich/hringbauer/git/punic_aDNA\n",
      "CPU Count: 28\n",
      "3.7.4 (default, Sep 11 2019, 11:24:51) \n",
      "[GCC 6.2.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os  # For Saving to Folder\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import socket\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import itertools as it\n",
    "from time import time\n",
    "\n",
    "# For Arial Font\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'sans-serif'   # Set the defaul\n",
    "# Make sure to have the font installed (it is on cluster for Harald)\n",
    "rcParams['font.sans-serif'] = ['Arial']\n",
    "\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "\n",
    "if socket_name.startswith(\"compute-\"):\n",
    "    print(\"HSM Computational partition detected.\")\n",
    "    path = \"/n/groups/reich/hringbauer/git/punic_aDNA/\"  # The Path on Midway Cluster\n",
    "else:\n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "\n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "# Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(os.getcwd())\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/n/groups/reich/DAVID/V49/V49.2/v49.2\"\n",
    "ind_path = base_path + \".ind\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Population File (what to pull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_pops(df, string, col=\"clst\", output=1):\n",
    "    \"\"\"Return list of clusters that contain string.\"\"\"\n",
    "    df1 = df[df[col].str.contains(string)]\n",
    "    \n",
    "    if output==1:\n",
    "        print(f\"{string}: {len(df1)} \")\n",
    "    elif output==2:\n",
    "        print(df1[col].value_counts())\n",
    "        \n",
    "    clsts = list(set(df1[col].values))\n",
    "    return clsts\n",
    "\n",
    "def run_convertf(path_convertf = \"./o2bin/convertf\", parfile = \"./explore_ntbk/parfiles/convertf.keep.par\"):\n",
    "    \"\"\"Runs the Downsampling\"\"\"\n",
    "    ! $path_convertf -p $parfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ind = pd.read_csv(ind_path, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "print(f\"Loaded {len(df_ind)} Individuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "### Testing for single Populations Populations\n",
    "return_pops(df_ind, string=\"Spain\", \n",
    "            output=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Ones v46.3\n",
    "version = \"49.2\"\n",
    "v0 = version.split(\".\")[0]\n",
    "\n",
    "pops = [\"Algeria\", \"Morocco\", \"Tunisia\", \"Punic\", \"Phoenician\", \"Spain_Vandal\", \"Spain_LBA\",\n",
    "        \"Sardinia\", \"Ibiza\", \"Israel_MLBA\", \"Israel_LBA\", \"Ashkelon\", \"Sicily\", \"Hellenistic\",\n",
    "        \"Israel_IA\", \"Israel_EIA\", \"Israel_Persian\", \"Gibraltar\", \"Lebanon\",\n",
    "        \"Spain_EBA_Afric\", \"Spain_BellBeaker_oAfrica\", \"Spain_Greek\",\n",
    "        \"Spain_Hellenistic\", \"Spain_IA\", \"Italy_Sardinia_C_oAfrica\", \n",
    "        \"Nigeria_IA\", \"Nigeria_Medieval\", \"Mallorca\", \"Menorca\", \n",
    "        \"Egypt_Hellenistic\", \"Egypt_Roman\", \"Egypt_Dynastic\", \"Egypt_Third\",\n",
    "        \"Spain_Roman_oAfrica2\", \"Formentera\", \"Aritgues\",  \"Greece_\", \"Guanche\", \"Israel_C\", \n",
    "        \"Spain_EN\", \"France_EN\"]\n",
    "\n",
    "exclude_strings = [\"_lc\", \"contam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsts = [return_pops(df_ind, string=pop, output=1) for pop in pops]\n",
    "\n",
    "clsts = [inner for ls in clsts for inner in ls]\n",
    "clsts = list(set(clsts)) # Filter to unique Elements\n",
    "print(f\"Loaded {len(clsts)} Populations\")\n",
    "\n",
    "### Exclude Strings\n",
    "for ex in exclude_strings:\n",
    "    clsts = [c for c in clsts if ex not in c]\n",
    "print(f\"After Exclusion {len(clsts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exclude Kerkouane in external sharing\n",
    "#clsts = [c for c in clsts if \"Tunisia_Punic\" not in c]\n",
    "#len(clsts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save List of populations to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsts = clsts + [\"include\"]\n",
    "#keep = np.array([\"Anatolia_N\", \"Iberia_HG\"])\n",
    "keep = np.array(clsts)\n",
    "path_keep = f\"./parfiles/keep_pops.v{version}\"\n",
    "np.savetxt(path_keep, keep, fmt=\"%s\")\n",
    "print(f\"Saved {len(clsts)} clusters to keep to: {path_keep}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flag out individuals who should not be extracted\n",
    "Idea: Some individuals should not be included in the final .ind file. To do this,\n",
    "I create a .ind file where the population of these is set to \"Ignore1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = f\"/n/groups/reich/DAVID/V{v0}/V{version}/v{version}\"\n",
    "save_path = f\"/n/groups/reich/hringbauer/Data/v{version}.all.flagged.ind\"\n",
    "\n",
    "ind_path = base_path + \".ind\"\n",
    "\n",
    "df_ind = pd.read_csv(ind_path, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "\n",
    "idx = df_ind[\"iid\"].str.endswith(\"_d\")\n",
    "df_ind.loc[idx, \"clst\"] = \"Ignore1\"\n",
    "print(f\"Flagged out {np.sum(idx)}/{len(idx)} downsampled Individuals\")\n",
    "df_ind.to_csv(save_path, header=False, sep=\" \", index=False)\n",
    "print(f\"Saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include Indivdiuals from Ilan's list.\n",
    "Include additional individuals from Ilans list.\n",
    "To do so set their clst to include (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path2 = f\"/n/groups/reich/hringbauer/Data/v{version}.all.flagged.included.ind\"\n",
    "\n",
    "df_add = pd.read_csv(\"./data/v49-added-samples.txt\", header=None, sep=r\"\\s+\", engine=\"python\")\n",
    "df_add.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "df_ind = pd.read_csv(save_path, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "\n",
    "### Add the additional Indivudals\n",
    "add_inds = [\"RISE507.508.merge.SG\", \"I13517_d\", \"I13518_d\", \"I13519_d\"] # Renamed indivdual plus some Myceneans\n",
    "search_inds = np.concatenate((df_add[\"iid\"], add_inds))\n",
    "\n",
    "idx = df_ind[\"iid\"].isin(search_inds)\n",
    "print(f\"Including {np.sum(idx)}/{len(search_inds)} IIDs from external source\")\n",
    "df_ind.loc[idx, \"clst\"] = \"include\"\n",
    "\n",
    "df_ind.to_csv(save_path2, header=False, sep=\" \", index=False)\n",
    "print(f\"Saved modified .ind file to: {save_path2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run convertf (with population list to keep)\n",
    "Additional parameters (such as position of output file) are coded into the parameter file\n",
    "\n",
    "Takes about 10 minutes for all individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_convertf(path_convertf = \"/n/groups/reich/hringbauer/o2bin/convertf\", \n",
    "             parfile = f\"./parfiles/qpAdm/convertf.anc_only.{version}.par\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge in Lazaridis Ancients with mergeit (with population list to keep)\n",
    "Takes about 3 min for v49.2\n",
    "Again: Some definitions are in the parfile. Please check/modify there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bin_merge_it = \"/n/groups/reich/hringbauer/o2bin/mergeit\"\n",
    "parfile_path = \"./parfiles/qpAdm/parMerge.v49.2\"\n",
    "\n",
    "! $bin_merge_it -p $parfile_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return to v49.2 Indivduals for extra included indivdiuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Original Indivdiaul\n",
    "save_path = f\"/n/groups/reich/hringbauer/Data/v{version}.all.flagged.ind\"\n",
    "df_org = pd.read_csv(save_path, delim_whitespace=True, header=None)\n",
    "df_org.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "\n",
    "save_path = f\"/n/groups/reich/hringbauer/Data/v{version}.all.flagged.ind\"\n",
    "df_org = pd.read_csv(save_path, delim_whitespace=True, header=None)\n",
    "df_org.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Individual File [Stand Alone from here]\n",
    "Overwrite Individuals with their individual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overwrite_ind_df(df, string, col=\"clst\", \n",
    "                     output=False, overwrite=\"\", iids=False):\n",
    "    \"\"\"Overwrite Individual Dataframe where col\n",
    "    contains string. Return modified dataframe (Copy)\n",
    "    where overwrite is the new Cluster ID\n",
    "    iids: Overwrite with IIDs if True!\"\"\"\n",
    "    idx = df[col].str.contains(string)\n",
    "    \n",
    "    if np.sum(idx)==0:\n",
    "        if output: \n",
    "            print(\"No Indivdiuals found\")\n",
    "        return\n",
    "    \n",
    "    if output:\n",
    "        print(f\"Found {np.sum(idx)} Matches\")\n",
    "        print(df[idx][col].value_counts())\n",
    "    \n",
    "    ### Actually  overwrite the Column\n",
    "    if len(overwrite)>0:\n",
    "        df.loc[idx, col] = overwrite\n",
    "        if output: \n",
    "            print(f\"{np.sum(idx)} Overwritten!\")\n",
    "            \n",
    "    if iids:\n",
    "        df.loc[idx, col] = df.loc[idx, \"iid\"] \n",
    "        \n",
    "        \n",
    "### Overwrite Individual IIDs\n",
    "def modifiy_iid_files(df_ind, pops_overwrite, \n",
    "                      pops_overwrite12=[], ind_modified=\"\"):\n",
    "    \"\"\"Modify .ind file. Overwrite individuals from pops_overwrite (list)\n",
    "    with their individuals labels. \n",
    "    df_int: Dataframe from Individuals.\n",
    "    pops_overwrite12: [[pop1,pop2]] list (nx2). Overwrites ALL string matches\n",
    "    for pop1 (contain) with pop2\"\"\"\n",
    "    \n",
    "    ### Overwrite with other Label\n",
    "    for pop1,pop2 in pops_overwrite12:\n",
    "        overwrite_ind_df(df_ind, pop1, overwrite=pop2)\n",
    "        \n",
    "    ### Overwrite with individual IIds\n",
    "    for pop in pops_overwrite:\n",
    "        overwrite_ind_df(df_ind, pop, \n",
    "                     iids=True, output=True)\n",
    "    \n",
    "    ### Save here\n",
    "    df_ind.to_csv(ind_modified, sep=\" \", index=None, header=False)\n",
    "    print(f\"Saved {len(df_ind)} Individuals to {ind_modified}\")\n",
    "    \n",
    "def set_clst_to_iid(df_ind, iids_overwrite, \n",
    "                    pops_overwrite12=[], savepath=\"\"):\n",
    "    \"\"\"Modify .ind file. Overwrite individuals from pops_overwrite (list)\n",
    "    with their individuals labels. \n",
    "    df_int: Dataframe from Individuals.\n",
    "    pops_overwrite12: [[pop1,pop2]] list (nx2). Overwrites ALL string matches\n",
    "    for pop1 (contain) with pop2\"\"\"\n",
    "    \n",
    "    ### Overwrite with other Label\n",
    "    idx = df_ind[\"iid\"].isin(iids)\n",
    "    print(f\"Overwriting {np.sum(idx)} Individuals\")\n",
    "    df_ind.loc[idx, \"clst\"] = df_ind.loc[idx, \"iid\"]\n",
    "        \n",
    "    ### Overwrite with other Label\n",
    "    for pop1,pop2 in pops_overwrite12:\n",
    "        overwrite_ind_df(df_ind, pop1, overwrite=pop2)\n",
    "    \n",
    "    ### Save here\n",
    "    if len(savepath)>0:\n",
    "        df_ind.to_csv(savepath, sep=\" \", index=None, header=False)\n",
    "        print(f\"Saved {len(df_ind)} Individuals to {savepath}\")\n",
    "        \n",
    "def set_iids_to_clst(df_ind, iids=[], clst=\"\", savepath=\"\"):\n",
    "    \"\"\"Set List of Indivdiuals to Cluster Label.\n",
    "    savepath: If defined: Save to there.\"\"\"\n",
    " \n",
    "    idx = df_ind[\"iid\"].isin(iids)\n",
    "    print(f\"Overwriting {np.sum(idx)} Individuals to {clst}\")\n",
    "    df_ind.loc[idx, \"clst\"] = clst\n",
    "    \n",
    "    ### Save here\n",
    "    if len(savepath)>0:\n",
    "        df_ind.to_csv(savepath, sep=\" \", index=None, header=False)\n",
    "        print(f\"Saved {len(df_ind)} Individuals to {savepath}\")  \n",
    "    return df_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1: Overwrite narrow target Individuals\n",
    "(For testing please see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_merged = f\"./eigenstrat/combined/punic.v{vrs}.ind\"          # What .ind to load\n",
    "df_ind = pd.read_csv(ind_merged, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "print(f\"Loaded {len(df_ind)} Individuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play around with Individual df to identify clusters of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ind[df_ind[\"clst\"].str.contains(\"Israel\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Only overwrite the Punics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Populations to overwrite. Typically because they have the \".SG\" label\n",
    "pops_overwrite12 = [[\"Morocco_LN\", \"Morocco_LN\"], [\"Morocco_EN\", \"Morocco_EN\"],\n",
    "                   [\"Morocco_Iberomaurusian\",\"Morocco_Iberomaurusian\"], [\"YRI\", \"YRI\"]]\n",
    "\n",
    "### Population to overwrite because they are the target\n",
    "pops_overwrite = [\"Punic\", \"Sicily_Punic\", \"Phoen\"] # \"Sardinia\", \"Spain\" \"Algeria\", \n",
    "\n",
    "ind_modified=\"./eigenstrat/combined/punic.v49.0_ind.ind\"    # Where to save the modified version to\n",
    "\n",
    "modifiy_iid_files(df_ind, pops_overwrite=pops_overwrite, \n",
    "                  pops_overwrite12=pops_overwrite12,\n",
    "                  ind_modified = ind_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Split up Iberian Bronze Age and Sardinian Nuragic too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pops_overwrite = [\"Algeria_N\", \"Punic\", \"Italy_Sicily_Phoenician\",\n",
    "                  \"Sicily_IA_Polizzello\", \"Sicani\", \"Phoen\",\n",
    "                  \"Morocco_LN\", \"Punic_oAfrican\", \n",
    "                  \"Iberia_North_BA_Africa_all\", \"Iberia_BA\", \"Iberia_IA\",\n",
    "                  \"Iberia_Greek\", \"Iberia_Hellenistic\",\n",
    "                  \"Iberia_BellBeaker_o\", \"Gibraltar_N\", \n",
    "                  \"Iberia_Iberian\", \"Iberia_Celtiberian\", \"Iberia_Tartessian\",\n",
    "                  \"Italy_Sardinia_C_o\", \"Nuragic\",\n",
    "                  \"Nigeria_IA\", \"Nigeria_Medieval\"\n",
    "                  ]\n",
    "\n",
    "pops_overwrite12 = [[\"Morocco_LN\", \"Morocco_LN\"], [\"Morocco_EN\", \"Morocco_EN\"],\n",
    "                   [\"Morocco_Iberomaurusian\",\"Morocco_Iberomaurusian\"], [\"YRI\",\"YRI\"]]\n",
    "\n",
    "ind_modified=\"./eigenstrat/combined/punic.v49.0_ind1.ind\"    # Where to save the modified version to\n",
    "\n",
    "modifiy_iid_files(df_ind, pops_overwrite=pops_overwrite, \n",
    "                  pops_overwrite12=pops_overwrite12,\n",
    "                  ind_modified = ind_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Modify all targets for distal modelling (Punics and Proximal Sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_merged=\"./eigenstrat/combined/punic.v49.0.ind\"          # What .ind to load\n",
    "df_ind = pd.read_csv(ind_merged, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "print(f\"Loaded {len(df_ind)} Individuals\")\n",
    "\n",
    "df_meta = pd.read_csv(\"/n/groups/reich/hringbauer/Data/v49.0.anno.csv\")\n",
    "dft = pd.merge(df_ind[\"iid\"], df_meta[[\"iid\", \"loc\", \"clst\", \"n_cov_snp\", \"age\", \"lat\", \"lon\"]], on=\"iid\")\n",
    "\n",
    "df_labels = pd.read_csv(\"./data/qpAdm_pops.tsv\", sep=\"\\t\")\n",
    "\n",
    "dfs = []\n",
    "for index, row in df_labels.iterrows():\n",
    "    dft1 = dft[dft[\"clst\"].str.contains(row[\"clst\"]) & (dft[\"loc\"] == row[\"loc\"])]\n",
    "    dfs.append(dft1)\n",
    "    \n",
    "df_targets = pd.concat(dfs)\n",
    "\n",
    "print(f\"Saving meta of {len(df_targets)} Individuals for further processing.\")\n",
    "#df_targets.to_csv(\"./output/tables/qpadm.targets.distal.v46.3.tsv\", sep=\"\\t\", index=False)\n",
    "iids = df_targets[\"iid\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only save if additional Individuals need a rerun\n",
    "df_temp = pd.read_csv(\"./output/tables/qpadm.targets.distal.v46.3.tsv\", sep=\"\\t\")\n",
    "idx = [iid not in df_temp[\"iid\"].values for iid in iids]\n",
    "#df_targets[idx].to_csv(\"./output/tables/qpadm.targets.distal.v46.3.add.tsv\", sep=\"\\t\")\n",
    "print(f\"Saved {np.sum(idx)} Indivdiuals for a rerun.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_clst_to_iid(df_ind, iids, pops_overwrite12=[], \n",
    "                savepath=\"./eigenstrat/combined/punic.v46.3_ind2.ind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Modify Individual PCA clusters of Punic Individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_merged=\"./eigenstrat/combined/punic.v49.0.ind\"          # What .ind to load\n",
    "df_ind = pd.read_csv(ind_merged, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "print(f\"Loaded {len(df_ind)} Individuals\")\n",
    "\n",
    "### Modify the Clusters\n",
    "iids_afr_punic =  [\"I18193\", \"I18189\", \"I22093\"]  # \"I22113\" high but not high enough\n",
    "iids_afr_cline = [\"I21966\", \"I21984\", \"I22094\", \"I22090\", \"VIL011\", \"VIL006\", \"VIL009\", \"VIL010\", \"VIL007\"] # but not VIL004\n",
    "\n",
    "df_ind = set_iids_to_clst(df_ind, iids=iids_afr_cline, clst=\"PunicCline\", savepath=\"\")\n",
    "df_ind = set_iids_to_clst(df_ind, iids=iids_afr_punic, clst=\"PunicAfrican\", savepath=\"\")\n",
    "\n",
    "savepath=\"./eigenstrat/combined/punic.v49.0_punic_clst.ind\"\n",
    "#df_ind.to_csv(savepath, sep=\" \", index=None, header=False)\n",
    "print(f\"Saved {len(df_ind)} Individuals to {savepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ind[df_ind[\"clst\"].str.contains(\"Phoen\")]\n",
    "\n",
    "#pops = [\"Spain_IA_Tartessian\", \"Spain_IA_Celt\", \"Italy_Sardinia_BA_Nuragic\", \n",
    "#        \"Italy_Sicily_IA_Sicani\", \"Greece_BA_Mycenaean\", \"Israel_Phoenician\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to David's Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.read_csv(\"/n/groups/reich/hringbauer/Data/Unpublished_data.tsv\", sep=\"\\t\")\n",
    "age_col = \"Average of 95.4% date range in calBP (defined as 1950 CE)  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.str.contains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_col = 'Group_ID (format convention which we try to adhere to is \"Country_<Geographic.Region_<Geographic.Subregion_>><Archaeological.Period.Or.DateBP_<Alternative.Archaeological.Period_>><Archaeological.Culture_<Alternative.Archaeological.Culture>><genetic.subgrouping.index.if.necessary_><\"o_\"sometimes.with.additional.detail.if.an.outlier><additional.suffix.especially.relative.status.if.we.recommend.removing.from.main.analysis.grouping><\"contam_\".if.contaminated><\"lc_\".if.<15000.SNPs.on.autosomal.targets><\".SG\".or.\".DG\".if.shotgun.data>; HG=hunter-gatherer, N=Neolithic, C=Chalcolithic/CopperAge, BA=BronzeAge, IA=IronAge, E=Early, M=Middle, L=Late, A=Antiquity)'\n",
    "groups = df_t[group_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsts_list = \"|\".join(clsts)\n",
    "idx = groups.str.contains(clsts_list)\n",
    "print(f\"Found {np.sum(idx)} Individuals\")\n",
    "df_found = df_t[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_found[150:174][[\"Master ID\", group_col, \"Publication\", \"Locality\", age_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_found.to_csv(\"./output/tables/samples_claim.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Entries in Eigenstrat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1320 Individuals\n"
     ]
    }
   ],
   "source": [
    "#ind_merged=\"./eigenstrat/anc_only.v46.3.ind\"          # What .ind to load\n",
    "ind_merged=\"./eigenstrat/combined/punic.v49.2.ind\"          # What .ind to load\n",
    "df_ind = pd.read_csv(ind_merged, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "print(f\"Loaded {len(df_ind)} Individuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "include    147\n",
       "Name: clst, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ind[df_ind[\"clst\"].str.contains(\"include\")][\"clst\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Outgroup: \n",
    "### Israel_MLBA Italy_Sardinia_EBA Spain_LBA Italy_Sicily_EBA Steppe_MLBA Greece_Minoan_Lassithi\n",
    "### Additional Source: Spain_IA I12433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ind[\"clst\"].value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ind[df_ind[\"clst\"].str.contains(\"Algeria\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
