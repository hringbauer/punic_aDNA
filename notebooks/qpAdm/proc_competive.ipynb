{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize and produce Tabular output for competitive qpAdm modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute-a-16-162.o2.rc.hms.harvard.edu\n",
      "HSM Computational partition detected.\n",
      "/n/groups/reich/hringbauer/git/punic_aDNA\n",
      "CPU Count: 32\n",
      "3.7.4 (default, Sep 11 2019, 11:24:51) \n",
      "[GCC 6.2.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os  # For Saving to Folder\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import socket\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import itertools as it\n",
    "from time import time\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# For Arial Font\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'sans-serif'   # Set the defaul\n",
    "# Make sure to have the font installed (it is on cluster for Harald)\n",
    "rcParams['font.sans-serif'] = ['Arial']\n",
    "\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "\n",
    "if socket_name.startswith(\"compute-\"):\n",
    "    print(\"HSM Computational partition detected.\")\n",
    "    path = \"/n/groups/reich/hringbauer/git/punic_aDNA/\"  # The Path on HMS Cluster\n",
    "else:\n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "\n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "# Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(os.getcwd())\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qp_adm(path):\n",
    "    \"\"\"Load, parse qpAdm .log-file.\n",
    "    Return: res, p_vals, pops, stds\n",
    "    path: Path of Input File\n",
    "    res: mxnx2 arryay: m: Nr subsets admixed pops, n: Nr \n",
    "    p_vals: List of m p-Values\n",
    "    pops: List of Strings of analyzed populations [target, s1, s2.., sn]\"\"\"\n",
    "    pop_line, pop_line_end = -1, -1 # Where the populations are found   \n",
    "    res_begin, res_end = -1, -1\n",
    "    std_line = -1  # Where the standard Deviation will land\n",
    "    \n",
    "    ### Parse the log file\n",
    "    # Iterate over everything and use the signals for start/stop\n",
    "    with open(path, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if line ==\"left pops:\\n\":\n",
    "                pop_line = i+1 \n",
    "            if line==\"right pops:\\n\":\n",
    "                pop_line_end = i-1 # There is an empty line before\n",
    "                \n",
    "            # Parse off everything to first space\n",
    "            s0 = line.split()\n",
    "            if len(s0)>=2:\n",
    "                if s0[0] == \"fixed\" and s0[1]==\"pat\":\n",
    "                    res_begin = i+1\n",
    "                    \n",
    "                if s0[0] == \"best\" and s0[1]==\"pat:\":\n",
    "                    if res_end < 0: # Only take the first occurence\n",
    "                        res_end = i\n",
    "                        \n",
    "                elif s0[0] == \"std.\" and s0[1]==\"errors:\":\n",
    "                    std_line = i\n",
    "    \n",
    "    ### Read out the results:\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        pops = lines[pop_line:pop_line_end]\n",
    "        pops = [p.rstrip() for p in pops]  # Chews off new line symbol\n",
    "\n",
    "        res = lines[res_begin:res_end]\n",
    "        stds = lines[std_line]\n",
    "    \n",
    "    ### Post-process the important Lines:\n",
    "    res_t = np.array([s.split()[:len(pops)+4] for s in res]) # 8 is valid for 3 pops!\n",
    "    res = res_t[:,5:].astype(\"float\")\n",
    "    p_vals = res_t[:,4].astype(\"float\")\n",
    "    stds = np.array(stds.split())[2:].astype(\"float\")  # Extract the Standard Deviations for the first line\n",
    "            \n",
    "    #### Return estimates and p-Value\n",
    "    assert(len(p_vals)==len(res)) # Sanity Check\n",
    "    return res, p_vals, pops, stds\n",
    "\n",
    "def load_qp_adm_1way(path):\n",
    "    \"\"\"Load, parse qpAdm .log-file for 1 way\n",
    "    path: Where to find the \n",
    "    Return p_value and pops:\n",
    "    p_value: List of m p-Values\n",
    "    pops: List of Strings of analyzed populations [target, s1]\"\"\"\n",
    "    pop_line, pop_line_end = -1, -1 # Where the populations are found   \n",
    "    res_line = -1\n",
    "    \n",
    "    ### Parse the log file\n",
    "    # Iterate over everything and use the signals for start/stop\n",
    "    with open(path, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if line ==\"left pops:\\n\":\n",
    "                pop_line = i+1 \n",
    "            if line==\"right pops:\\n\":\n",
    "                pop_line_end = i-1 # There is an empty line before\n",
    "                \n",
    "            # Parse off everything to first space\n",
    "            s0 = line.split()\n",
    "            if len(s0)>=2:\n",
    "                if s0[0] == \"f4rank:\" and s0[1] == \"1\":\n",
    "                    res_line = i\n",
    "    \n",
    "    ### Read out the results:\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        pops = lines[pop_line:pop_line_end]\n",
    "        pops = [p.rstrip() for p in pops]  # Chews off new line symbol\n",
    "        res = lines[res_line]\n",
    "    \n",
    "    ### Post-process the important Lines:\n",
    "    p_val = float(res.split()[-1])\n",
    "    #### Return estimates and p-Value\n",
    "    return p_val, pops\n",
    "\n",
    "\n",
    "def create_admix_df(source_pops, admix_coeffs, stds, p_vals):\n",
    "    \"\"\"Create and return dataframe with Admixture Proportions and Standard Errors.\n",
    "    Assume all input is as Numpy Array (otherwise indexing errors!!)\"\"\"\n",
    "    n = len(source_pops[0,:])  # Get Number Sources +1\n",
    "    df = pd.DataFrame({\"target\":source_pops[:,0], \"p-Value\":p_vals})\n",
    "    \n",
    "    for i in range(1,n):\n",
    "        df[f\"Source_{i}\"] = source_pops[:,i]\n",
    "        \n",
    "    for i in range(1,n):\n",
    "        df[f\"Fraction_{i}\"] = admix_coeffs[:,i-1]\n",
    "        \n",
    "    for i in range(1,n):\n",
    "        df[f\"STD_{i}\"] = stds[:,i-1]\n",
    "    return df\n",
    "    \n",
    "def give_admix0(res, minval=-1e-4):\n",
    "    \"\"\"Return the Admixture Coefficients of the first Model.\n",
    "    Only return 0 if feasible, else np.nan\"\"\"\n",
    "    \n",
    "    feasible = np.min(res[0])>minval  # Check if feasible\n",
    "    if not feasible:\n",
    "        return np.nan  # No Feasible Model\n",
    "    return 0\n",
    "    \n",
    "def give_admix_index(res):\n",
    "    \"\"\"Return index of first viable admixture result.\n",
    "    res: nxk array. n...Nr of all subsets, k...Nr of Source Pops\"\"\"    \n",
    "    for i, ls in enumerate(res):\n",
    "        if np.min(ls)>-1e-4:\n",
    "            return i\n",
    "    print(\"Warning: No valid admixture found!!\")       \n",
    "    return np.nan # Default Return\n",
    "\n",
    "def give_admix_index_best(res, pvals, minval=-1e-4):\n",
    "    \"\"\"Return index of best viable admixture result.\n",
    "    res: nxk array. n...Nr of all subsets, k...Nr of Source Pops\"\"\"  \n",
    "    feasible = np.min(res, axis=1)>minval  # Extract all feasible results\n",
    "    pvals_okay = feasible * pvals  # Set bad ones to 0.\n",
    "    \n",
    "    if np.max(pvals_okay)==0:\n",
    "        print(\"Warning: No valid admixture found!!\")\n",
    "        return np.nan\n",
    "    \n",
    "    i = np.argmax(pvals_okay) # The Index with the Maximum Value\n",
    "    return i\n",
    "    \n",
    "def sci_notation(num, decimal_digits=1, precision=None, exponent=None):\n",
    "    \"\"\"\n",
    "    Returns a string representation of the scientific\n",
    "    notation of the given number formatted for use with\n",
    "    LaTeX or Mathtext, with specified number of significant\n",
    "    decimal digits and precision (number of decimal digits\n",
    "    to show). The exponent to be used can also be specified\n",
    "    explicitly.\n",
    "    \"\"\"\n",
    "    if not exponent:\n",
    "        exponent = int(np.floor(np.log10(abs(num))))\n",
    "    coeff = round(num / float(10**exponent), decimal_digits)\n",
    "    if not precision:\n",
    "        precision = decimal_digits\n",
    "\n",
    "    return \"${0:.{2}f}\\cdot10^{{{1:d}}}$\".format(coeff, exponent, precision)\n",
    "\n",
    "\n",
    "def create_latex_lines(source_pops, admix_coeffs, stds, p_vals, na = \"-\", rp = \"A12\"):\n",
    "    \"\"\"Print a Line for Latex Table in the main Text. \n",
    "    Script to speed things up.\n",
    "    source_pops, nx(j+1) Array, j...NR of Sources (<=3)\n",
    "    admix_coeffs, stds: nxj Array\n",
    "    p_vals: nx1 Array\n",
    "    na: Character for Missing Data\n",
    "    rp: String for right population\"\"\"\n",
    "    out =\"\"\n",
    "    \n",
    "    for i in range(len(source_pops)): # Iterate over every line\n",
    "        ls = [na for _ in range(12)] # Create empty vector with na Symbol\n",
    "        \n",
    "        assert(len(source_pops[i])<=4) # At most 4 sources\n",
    "        \n",
    "        for j, s in enumerate(source_pops[i]): # Fill in the Source Populations\n",
    "            ls[j] = s   \n",
    "            \n",
    "        ls[4] = rp        # Right Pop\n",
    "        \n",
    "        p = p_vals[i]\n",
    "        \n",
    "        if p==0.0:\n",
    "            ls[5] = \"0\" \n",
    "        elif p>=0.05:\n",
    "            ls[5] = \"\\\\textbf{\" +  str(np.around(p, 3)) + \"}\" # Make bold\n",
    "            \n",
    "        elif p>= 0.01:\n",
    "            ls[5] = np.around(p, 3) # Round to three Digits            \n",
    "        elif p<0.01: \n",
    "            ls[5] = sci_notation(p, decimal_digits=1)  # Do proper formatting      \n",
    "        \n",
    "        for j, x in enumerate(admix_coeffs[i]): # Fill in the admixture fractions\n",
    "            ls[6+j] = \"{:.3f}\".format(x) \n",
    "            \n",
    "        for j, x in enumerate(stds[i]): # Fill in the uncertainty of admixture fractions\n",
    "            ls[9+j] = \"{:.3f}\".format(x)        \n",
    "        \n",
    "        s = \" & \".join(str(x) for x in ls) # Convert to Strings and Join\n",
    "        s=s.replace(\"_\", \"-\") # Replace tricky underscore symbols (for Latex tables)\n",
    "        print(s + \"\\\\\\\\\") # Add two backslashes\n",
    "        out += (s + \"\\n\") # Do next Line\n",
    "        \n",
    "    return out # Return the full text\n",
    "\n",
    "def load_iids_from_indfile(path_ind, string, \n",
    "                           col=\"clst\", col_iid=\"iid\",\n",
    "                           iids_okay=[]):\n",
    "    \"\"\"Load IIDs from Ind File\n",
    "    Return List of IIDs\"\"\"\n",
    "    df_ind = pd.read_csv(path_ind, delim_whitespace=True, header=None)\n",
    "    df_ind.columns=[\"iid\", \"sex\",\"clst\"]\n",
    "    idx = df_ind[col].str.contains(string)\n",
    "    ls = df_ind[idx][col_iid].values\n",
    "    \n",
    "    ### If needed filter out okay Individuals\n",
    "    if len(iids_okay)>0:\n",
    "        ls = np.intersect1d(ls, iids_okay)\n",
    "    return ls\n",
    "\n",
    "###################################################\n",
    "### Load okay Individual IIDs\n",
    "\n",
    "def load_individuals_filetered(\n",
    "                            path_anno = \"/n/groups/reich/hringbauer/Data/v42.3.anno.csv\",\n",
    "                            col=\"clst\", col_iid=\"iid\",\n",
    "                            min_snps_cov=50000,\n",
    "                            snp_cov_col=\"n_cov_snp\",\n",
    "                            master_id_col=\"Master ID\",\n",
    "                            ):\n",
    "    \"\"\"Filter List of Individuals against meta,\n",
    "    using minimal Nr of SNPs and unique IDs\"\"\"\n",
    "\n",
    "    df_all = pd.read_csv(path_anno)\n",
    "\n",
    "    ### Keep only the best coerage Indivdual\n",
    "    df_all = df_all.sort_values(by=snp_cov_col, ascending=False)\n",
    "    df_all = df_all.drop_duplicates(subset=master_id_col)\n",
    "    \n",
    "    ### Filter to min Nr of SNPs\n",
    "    df_all = df_all[df_all[snp_cov_col]>=min_snps_cov]\n",
    "    \n",
    "    ### \n",
    "    df_all = df_all\n",
    "    return df_all[\"iid\"].values\n",
    "\n",
    "######################################\n",
    "### Helper Functions\n",
    "\n",
    "def create_empty_qpAdm_df(k, n_raws=0):\n",
    "    \"\"\"Return qpAdm dataframe\n",
    "    for k sources.\n",
    "    n_entries: How many empty Entries \"\"\"\n",
    "    df = pd.DataFrame({})\n",
    "    df[\"t\"] = np.nan\n",
    "    df[\"p\"] = np.nan\n",
    "    df[\"n\"] = np.nan\n",
    "\n",
    "    ### Create an empty data frame\n",
    "    for i in range(1, k+1):\n",
    "        df[\"s\" + str(i)] = np.nan\n",
    "        \n",
    "    for i in range(1, k+1):\n",
    "        df[\"f\" + str(i)] = np.float(np.nan)\n",
    "\n",
    "    for i in range(1, k+1):\n",
    "        df[\"std\" + str(i)] = np.float(np.nan)\n",
    "    \n",
    "    ### Add empty rows\n",
    "    if n_raws>0:\n",
    "        df = df.reindex(df.index.tolist() + list(range(n_raws)))\n",
    "        #df[\"p\"] = df[\"p\"].astype(\"float\")\n",
    "        #df[\"n\"] = df[\"n\"].astype(\"float\")\n",
    "        #for i in range(1, k+1):\n",
    "        #    df[\"f\" + str(i)] = df[\"f\" + str(i)].astype(\"float\")\n",
    "        #    df[\"std\" + str(i)] = df[\"std\" + str(i)].astype(\"float\")\n",
    "    return df\n",
    "\n",
    "def load_first_qpAdm_model(path):\n",
    "    \"\"\"Parse and return the first values of qpAdm model\n",
    "    from log-file at path. \n",
    "    Return mixture coefficients, palue, left pops\n",
    "    and standard errors\"\"\"\n",
    "    res, p_vals, pops, stds =  load_qp_adm(path)\n",
    "\n",
    "    ### Only Keep the first results\n",
    "    res = res[0]\n",
    "    p_val = p_vals[0] \n",
    "    return res, p_val, pops, stds\n",
    "\n",
    "def set_feasible_df(df, sources=5):\n",
    "    \"\"\"Set Feasible Models\"\"\"\n",
    "    \n",
    "    fs = [f\"f{i}\" for i in range(1,sources+1)]\n",
    "\n",
    "    idx_if = np.nanmin(df[fs], axis=1) < 0\n",
    "    df[\"fs\"] = True\n",
    "    df.loc[idx_if, \"fs\"] = False\n",
    "    \n",
    "    print(f\"Set {np.sum(idx_if)}/{len(idx_if)} infeasible models\")\n",
    "    return df\n",
    "\n",
    "def get_individual_models(df, min_p=0.01):\n",
    "    \"\"\"Get Dataframe with rows of best Indivdidual Model\n",
    "    (lowest number of sources, highest p Value)\"\"\"\n",
    "    #df = df.sort_values(by=[\"p\", \"n\"], ascending=[False, True])\n",
    "    dft = df[df[\"p\"]>=min_p]\n",
    "    dft = dft.sort_values(by=[\"n\", \"p\"], ascending=[True, False])\n",
    "    \n",
    "    idx_dup = dft[\"t\"].duplicated()\n",
    "    dft = dft[~idx_dup]\n",
    "    return dft.reset_index(drop=True).copy()\n",
    "\n",
    "def reorder_qpadm_df(df, sources=[]):\n",
    "    \"\"\"Reorder qpAdm Dataframe, so that sources \n",
    "    are in consistent order. Only changes columns.\n",
    "    Return updated dataframe\"\"\"\n",
    "    df1 = df.copy() # Create Copy\n",
    "    df1.iloc[:,:] = 0  # set everything to 0\n",
    "    df1[\"p\"] = df[\"p\"] # Fill inmutables\n",
    "    df1[\"t\"] = df[\"t\"]\n",
    "    df1[\"n\"] = df[\"n\"]\n",
    "\n",
    "    for i,s in enumerate(sources):\n",
    "        i1=i+1 # Move Index one up to start filling at 1\n",
    "        #df1[f\"s{i1}\"] = s  # Dont fill in if not in model\n",
    "        for j in range(1,len(sources)+1):\n",
    "            idx = df[f\"s{j}\"] == s\n",
    "            df1.loc[idx, f\"s{i1}\"] = df.loc[idx, f\"s{j}\"] # Only fill in where data\n",
    "            df1.loc[idx, f\"f{i1}\"] = df.loc[idx, f\"f{j}\"]\n",
    "            df1.loc[idx, f\"std{i1}\"] = df.loc[idx, f\"std{j}\"]\n",
    "            \n",
    "    assert(~np.sum(df1[df1[\"n\"]>1].isnull()).all())\n",
    "    return df1.copy()\n",
    "\n",
    "def get_df_n_way_model(ns=[2,], comp_groups=[], targets=[], \n",
    "                       base_folder=\"\", reorder_sources=[]):\n",
    "    \"\"\"Get Dataframe of all competative n_way models of iids.\n",
    "    ns: List of n i n-way models.\n",
    "    comp_groups: The competivie right groups.\n",
    "    targets: List of target iids\n",
    "    base_folder: where to find the qpadm output .log files.\n",
    "    reorder_sources: If given reorder sources\n",
    "    Return qpAdm dataframe\"\"\"\n",
    "    k = len(comp_groups) # For verall size of results dataframe\n",
    "    k1 = len(targets)\n",
    "    \n",
    "    dfs = []\n",
    "    for n in ns: # Iterate over number of sources\n",
    "        source_list = np.array(list(it.combinations(comp_groups, r=n)))\n",
    "        l = len(source_list)\n",
    "        \n",
    "        df = create_empty_qpAdm_df(k, n_raws=l*k1)\n",
    "        df[\"n\"] = n\n",
    "        \n",
    "        i = 0\n",
    "        for iid in targets:  # iterate over targets\n",
    "            for s in source_list: # iterate over number of sources\n",
    "                path_log = \".\".join([iid] + list(s)) + \".log\"\n",
    "                path_load = f\"{base_folder}{n}way/{path_log}\"\n",
    "                \n",
    "                if n==1:\n",
    "                    p_val, pops = load_qp_adm_1way(path_load)\n",
    "                    res, stds = [1], [0] # No uncertainty about 1way model\n",
    "                    \n",
    "                elif n>1:\n",
    "                    res, p_val, pops, stds = load_first_qpAdm_model(path_load)\n",
    "                    \n",
    "                ### Fill dataframe row\n",
    "                df.loc[i,\"t\"] = pops[0]\n",
    "                df.loc[i,\"p\"] = p_val\n",
    "\n",
    "                for l in range(n):\n",
    "                    l1 = l + 1  \n",
    "                    df.loc[i,f\"s{l1}\"] = pops[l1]\n",
    "                    df.loc[i,f\"f{l1}\"] = res[l]\n",
    "                    df.loc[i,f\"std{l1}\"] = stds[l]\n",
    "                i+= 1 # Jump to the row\n",
    "                \n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs)\n",
    "    \n",
    "    if len(reorder_sources)>0:\n",
    "        df = reorder_qpadm_df(df, order_new = reorder_sources)\n",
    "        \n",
    "    df = set_feasible_df(df, sources=len(comp_groups))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def reorder_qpAdm_df2(df, order_new=[]):\n",
    "    \"\"\"Reorder qpAdm df so that \n",
    "    s1,s2,..sn are in order of order\n",
    "    order: List of populations [s1,s2,...,sn]\"\"\"\n",
    "    \n",
    "    df1 = df.copy() # Placeholder for the new Dataframe\n",
    "    \n",
    "    for n, s in enumerate(order_new):    \n",
    "        n1 = n + 1  # Index of pop starts at 1\n",
    "        df1[f\"s{n1}\"]=np.nan\n",
    "        df1[f\"f{n1}\"] = 0\n",
    "        df1[f\"std{n1}\"] = 0\n",
    "        \n",
    "        for c1 in range(1,len(order_new)+1): # Go over all columns\n",
    "            idx = df[f\"s{c1}\"] == s\n",
    "        \n",
    "            df1.loc[idx, f\"s{n1}\"] = df.loc[idx, f\"s{c1}\"]\n",
    "            df1.loc[idx, f\"f{n1}\"] = df.loc[idx, f\"f{c1}\"]\n",
    "            df1.loc[idx, f\"std{n1}\"] = df.loc[idx, f\"std{c1}\"]\n",
    "    return df1\n",
    "        \n",
    "def get_spec_model(df, sources=[], tot_sources=5, \n",
    "                   drop=True, output=True):\n",
    "    \"\"\"Get specific model for dataframe df.\n",
    "    Return dataframe with this model.\"\"\"\n",
    "    k = len(sources)\n",
    "    found = np.ones(len(df), dtype=\"bool\")\n",
    "    for i in range(1,k+1):\n",
    "        idx = df[f\"s{i}\"].isin(sources)\n",
    "        found = found & idx\n",
    "    for i in range(k+1, tot_sources+1): \n",
    "        idx = df[f\"s{i}\"].isnull()\n",
    "        found = found & idx\n",
    "        \n",
    "    if output: \n",
    "        print(f\"Found {np.sum(found)} fitting rows.\")\n",
    "    dft = df[found]\n",
    "    \n",
    "    ### Drop Unnecessary Labels\n",
    "    if drop:\n",
    "        for i in range(k+1, tot_sources+1):\n",
    "            dft = dft.drop(columns=[f\"s{i}\", f\"f{i}\", f\"std{i}\"])\n",
    "    \n",
    "    return dft.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 Punic IIDs\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "reorder_qpadm_df() got an unexpected keyword argument 'order_new'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-8164f06d00f7>\u001b[0m in \u001b[0;36mget_df_n_way_model\u001b[0;34m(ns, comp_groups, targets, base_folder, reorder_sources)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreorder_sources\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreorder_qpadm_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreorder_sources\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_feasible_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomp_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: reorder_qpadm_df() got an unexpected keyword argument 'order_new'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "comp_groups = [\"Italy_Sicily_IA_Polizzello\", \"Algeria_IA\", \"Greece_BA_Mycenaean\", \n",
    "               \"Israel_Phoenician\", \"Italy_Sardinia_BA_Nuragic\", \"Spain_IA\"]\n",
    "\n",
    "reorder_sources = [\"Algeria_IA\", \"Italy_Sicily_IA_Polizzello\", \"Italy_Sardinia_BA_Nuragic\", \n",
    "              \"Spain_IA\", \"Greece_BA_Mycenaean\",  \"Israel_Phoenician\"]\n",
    "\n",
    "base_folder = \"./output/qpAdm/comp2.v46.3/\"\n",
    "\n",
    "### Get the IIDs\n",
    "df = pd.read_csv(\"./output/tables/qpAdm30Kpunic.v46.3.tsv\", sep=\"\\t\")\n",
    "iids = df[\"iid\"].values[:2]\n",
    "d = np.array([\"S18200.Y1.E2.L1\", 'MS10614.SG']) # Bad qpAdm runs?!\n",
    "iids= np.setdiff1d(iids, d)\n",
    "print(f\"Loaded {len(iids)} Punic IIDs\")\n",
    "\n",
    "df = get_df_n_way_model(ns=[1,2,3,4], comp_groups=comp_groups, \n",
    "                        targets=iids[:], base_folder=base_folder, \n",
    "                        reorder_sources=reorder_sources)\n",
    "\n",
    "### Save the Data to tabular Format\n",
    "df.to_csv(\"./output/qpAdm/v46.3/ind_model_comp2r.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1b) Process the proximal model with reordering\n",
    "This is needed for plotting - as it produces a standardized dataframe with all sources ordered and also single models filled in correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./output/qpAdm/v46.3/ind_model_comp2.tsv\", sep=\"\\t\")\n",
    "\n",
    "new_groups = [\"Algeria_IA\", \"Italy_Sicily_IA_Polizzello\", \"Italy_Sardinia_BA_Nuragic\", \n",
    "              \"Spain_IA\", \"Greece_BA_Mycenaean\",  \"Israel_Phoenician\"]\n",
    "\n",
    "df.to_csv(\"./output/qpAdm/v46.3/ind_model_comp2r.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to play with: Analyze the best Individual Models\n",
    "Not needed downstream, only gives out data in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3875 qpAdm models.\n",
      "Set 2006/3875 infeasible models\n",
      "Got 76 best Indivdiuals models\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "\n",
    "df = pd.read_csv(\"./output/qpAdm/v46.3/ind_model_comp.tsv\", sep=\"\\t\")\n",
    "print(f\"Loaded {len(df)} qpAdm models.\")\n",
    "\n",
    "df = set_feasible_df(df, sources=5)\n",
    "df1 = df[df[\"fs\"]==1].copy()\n",
    "df2 = df1.copy()\n",
    "df_ind = get_individual_models(df1, min_p=0.01)\n",
    "print(f\"Got {len(df_ind)} best Indivdiuals models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7000 qpAdm models.\n",
      "Set 3643/7000 infeasible models\n",
      "Got 115 best Indivdiuals models\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "### Load the wave 2 model\n",
    "df = pd.read_csv(\"./output/qpAdm/v46.3/ind_model_comp2.tsv\", sep=\"\\t\")\n",
    "print(f\"Loaded {len(df)} qpAdm models.\")\n",
    "\n",
    "df = set_feasible_df(df, sources=6)\n",
    "df1 = df[df[\"fs\"]==1].copy() ### Only feasible\n",
    "df2 = df1.copy()\n",
    "df_ind2 = get_individual_models(df1, min_p=0.01)\n",
    "print(f\"Got {len(df_ind2)} best Indivdiuals models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all Results for one individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7000 qpAdm models.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./output/qpAdm/v46.3/ind_model_comp2.tsv\", sep=\"\\t\")\n",
    "print(f\"Loaded {len(df)} qpAdm models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1[\"t\"]==\"I22113\"].sort_values(by=\"p\", ascending=False)#[[\"f1\", \"f2\", \"f3\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all Results for one specific Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 94 fitting rows.\n",
      "Fitting Models at p>0.01: 72\n"
     ]
    }
   ],
   "source": [
    "df2 = get_spec_model(df1, sources=[\"Greece_BA_Mycenaean\", \"Spain_IA\", \"Algeria_IA\"])   # \"Israel_Phoenician\"\n",
    "s = np.sum(df2[\"p\"]>0.01)\n",
    "print(f\"Fitting Models at p>0.01: {s}\")\n",
    "#Italy_Sicily_IA_Polizzello  Italy_Sardinia_BA_Nuragic\tTunisia_N Greece_BA_Mycenaean Israel_Phoenician"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting at p>0.01\n",
      "\n",
      "Sources: 1\n",
      "('Italy_Sicily_IA_Polizzello',): 4\n",
      "('Algeria_IA',): 3\n",
      "('Greece_BA_Mycenaean',): 9\n",
      "('Israel_Phoenician',): 6\n",
      "('Italy_Sardinia_BA_Nuragic',): 0\n",
      "('Spain_IA',): 4\n",
      "\n",
      "Sources: 2\n",
      "('Italy_Sicily_IA_Polizzello', 'Algeria_IA'): 34\n",
      "('Italy_Sicily_IA_Polizzello', 'Greece_BA_Mycenaean'): 8\n",
      "('Italy_Sicily_IA_Polizzello', 'Israel_Phoenician'): 28\n",
      "('Italy_Sicily_IA_Polizzello', 'Italy_Sardinia_BA_Nuragic'): 0\n",
      "('Italy_Sicily_IA_Polizzello', 'Spain_IA'): 7\n",
      "('Algeria_IA', 'Greece_BA_Mycenaean'): 51\n",
      "('Algeria_IA', 'Israel_Phoenician'): 14\n",
      "('Algeria_IA', 'Italy_Sardinia_BA_Nuragic'): 6\n",
      "('Algeria_IA', 'Spain_IA'): 9\n",
      "('Greece_BA_Mycenaean', 'Israel_Phoenician'): 11\n",
      "('Greece_BA_Mycenaean', 'Italy_Sardinia_BA_Nuragic'): 8\n",
      "('Greece_BA_Mycenaean', 'Spain_IA'): 26\n",
      "('Israel_Phoenician', 'Italy_Sardinia_BA_Nuragic'): 15\n",
      "('Israel_Phoenician', 'Spain_IA'): 45\n",
      "('Italy_Sardinia_BA_Nuragic', 'Spain_IA'): 2\n",
      "\n",
      "Sources: 3\n",
      "('Italy_Sicily_IA_Polizzello', 'Algeria_IA', 'Greece_BA_Mycenaean'): 44\n",
      "('Italy_Sicily_IA_Polizzello', 'Algeria_IA', 'Israel_Phoenician'): 55\n",
      "('Italy_Sicily_IA_Polizzello', 'Algeria_IA', 'Italy_Sardinia_BA_Nuragic'): 4\n",
      "('Italy_Sicily_IA_Polizzello', 'Algeria_IA', 'Spain_IA'): 31\n",
      "('Italy_Sicily_IA_Polizzello', 'Greece_BA_Mycenaean', 'Israel_Phoenician'): 10\n",
      "('Italy_Sicily_IA_Polizzello', 'Greece_BA_Mycenaean', 'Italy_Sardinia_BA_Nuragic'): 2\n",
      "('Italy_Sicily_IA_Polizzello', 'Greece_BA_Mycenaean', 'Spain_IA'): 9\n",
      "('Italy_Sicily_IA_Polizzello', 'Israel_Phoenician', 'Italy_Sardinia_BA_Nuragic'): 8\n",
      "('Italy_Sicily_IA_Polizzello', 'Israel_Phoenician', 'Spain_IA'): 43\n",
      "('Italy_Sicily_IA_Polizzello', 'Italy_Sardinia_BA_Nuragic', 'Spain_IA'): 1\n",
      "('Algeria_IA', 'Greece_BA_Mycenaean', 'Israel_Phoenician'): 18\n",
      "('Algeria_IA', 'Greece_BA_Mycenaean', 'Italy_Sardinia_BA_Nuragic'): 37\n",
      "('Algeria_IA', 'Greece_BA_Mycenaean', 'Spain_IA'): 72\n",
      "('Algeria_IA', 'Israel_Phoenician', 'Italy_Sardinia_BA_Nuragic'): 24\n",
      "('Algeria_IA', 'Israel_Phoenician', 'Spain_IA'): 45\n",
      "('Algeria_IA', 'Italy_Sardinia_BA_Nuragic', 'Spain_IA'): 6\n",
      "('Greece_BA_Mycenaean', 'Israel_Phoenician', 'Italy_Sardinia_BA_Nuragic'): 11\n",
      "('Greece_BA_Mycenaean', 'Israel_Phoenician', 'Spain_IA'): 36\n",
      "('Greece_BA_Mycenaean', 'Italy_Sardinia_BA_Nuragic', 'Spain_IA'): 10\n",
      "('Israel_Phoenician', 'Italy_Sardinia_BA_Nuragic', 'Spain_IA'): 38\n",
      "\n",
      "Sources: 4\n",
      "('Italy_Sicily_IA_Polizzello', 'Algeria_IA', 'Greece_BA_Mycenaean', 'Israel_Phoenician'): 6\n",
      "('Italy_Sicily_IA_Polizzello', 'Algeria_IA', 'Greece_BA_Mycenaean', 'Italy_Sardinia_BA_Nuragic'): 5\n",
      "('Italy_Sicily_IA_Polizzello', 'Algeria_IA', 'Greece_BA_Mycenaean', 'Spain_IA'): 30\n",
      "('Italy_Sicily_IA_Polizzello', 'Algeria_IA', 'Israel_Phoenician', 'Italy_Sardinia_BA_Nuragic'): 14\n",
      "('Italy_Sicily_IA_Polizzello', 'Algeria_IA', 'Israel_Phoenician', 'Spain_IA'): 60\n",
      "('Italy_Sicily_IA_Polizzello', 'Algeria_IA', 'Italy_Sardinia_BA_Nuragic', 'Spain_IA'): 3\n",
      "('Italy_Sicily_IA_Polizzello', 'Greece_BA_Mycenaean', 'Israel_Phoenician', 'Italy_Sardinia_BA_Nuragic'): 2\n",
      "('Italy_Sicily_IA_Polizzello', 'Greece_BA_Mycenaean', 'Israel_Phoenician', 'Spain_IA'): 12\n",
      "('Italy_Sicily_IA_Polizzello', 'Greece_BA_Mycenaean', 'Italy_Sardinia_BA_Nuragic', 'Spain_IA'): 1\n",
      "('Italy_Sicily_IA_Polizzello', 'Israel_Phoenician', 'Italy_Sardinia_BA_Nuragic', 'Spain_IA'): 14\n",
      "('Algeria_IA', 'Greece_BA_Mycenaean', 'Israel_Phoenician', 'Italy_Sardinia_BA_Nuragic'): 10\n",
      "('Algeria_IA', 'Greece_BA_Mycenaean', 'Israel_Phoenician', 'Spain_IA'): 43\n",
      "('Algeria_IA', 'Greece_BA_Mycenaean', 'Italy_Sardinia_BA_Nuragic', 'Spain_IA'): 31\n",
      "('Algeria_IA', 'Israel_Phoenician', 'Italy_Sardinia_BA_Nuragic', 'Spain_IA'): 46\n",
      "('Greece_BA_Mycenaean', 'Israel_Phoenician', 'Italy_Sardinia_BA_Nuragic', 'Spain_IA'): 21\n"
     ]
    }
   ],
   "source": [
    "comp_groups = [\"Italy_Sicily_IA_Polizzello\", \"Algeria_IA\", \"Greece_BA_Mycenaean\", \n",
    "               \"Israel_Phoenician\", \"Italy_Sardinia_BA_Nuragic\", \"Spain_IA\"]\n",
    "p=0.01\n",
    "\n",
    "print(f\"Fitting at p>{p}\")\n",
    "for r in range(1,5):\n",
    "    print(f\"\\nSources: {r}\")\n",
    "    for ls in it.combinations(comp_groups, r=r):\n",
    "        df2 = get_spec_model(df1, sources=ls, output=False)   # \"Israel_Phoenician\"\n",
    "        s = np.sum(df2[\"p\"]>p)\n",
    "        print(f\"{ls}: {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65 fitting rows.\n"
     ]
    }
   ],
   "source": [
    "df2 = get_spec_model(df1, sources=['Italy_Sicily_IA_Polizzello', 'Algeria_IA', 'Israel_Phoenician', 'Spain_IA']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sort_values(by=\"p\",ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"t\"]==\"I21984\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all Results for one Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv(\"/n/groups/reich/hringbauer/Data/v46.3.anno.csv\")\n",
    "dft = pd.merge(df1, df_meta[[\"iid\", \"loc\", \"clst\", \"n_cov_snp\"]], left_on=\"t\", right_on=\"iid\", how=\"left\")\n",
    "dft = get_individual_models(dft, min_p=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft[dft[\"loc\"].str.contains(\"Tharros\")].sort_values(by=\"p\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sort_values(by=\"p\", ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "ax = plt.gca()\n",
    "ax.hist(df2[\"p\"], ec=\"k\", bins=21)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the Distal Model with Tunisia N\n",
    "Takes about 6 min for 173 Indivdiuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 193 IIDs for distal modelling\n",
      "Set 6942/12159 infeasible models\n",
      "CPU times: user 1min 51s, sys: 4.02 s, total: 1min 55s\n",
      "Wall time: 8min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "comp_groups = [\"Anatolia_N\", \"Steppe_MLBA\", \"WHG\", \n",
    "               \"Tunisia_N\", \"Levant_N\", \"Iran_N\"]\n",
    "\n",
    "base_folder = \"./output/qpAdm/dist.v46.3/\"\n",
    "\n",
    "### Get the IIDs\n",
    "df = pd.read_csv(\"./output/tables/qpadm.targets.distal.v46.3.tsv\", sep=\"\\t\")\n",
    "iids = df[\"iid\"].values\n",
    "d = np.array([\"S18200.Y1.E2.L1\", 'MS10614.SG']) # Bad qpAdm runs?!\n",
    "iids= np.setdiff1d(iids, d)\n",
    "print(f\"Loaded {len(iids)} IIDs for distal modelling\")\n",
    "\n",
    "df = get_df_n_way_model(ns=[1,2,3,4,5,6], comp_groups=comp_groups, \n",
    "                        targets=iids[:], \n",
    "                        base_folder=base_folder, reorder_sources=comp_groups)\n",
    "\n",
    "### Save the Data to tabular Format\n",
    "df.to_csv(\"./output/qpAdm/v46.3/ind_model_dist.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the distal model with Algeria IA\n",
    "Same as above but folders different\n",
    "Per 20 individuals: ~ XX min \n",
    "(in total 193)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 192 IIDs for distal modelling\n",
      "Set 6897/12096 infeasible models\n",
      "CPU times: user 59.1 s, sys: 1.93 s, total: 1min\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "comp_groups = [\"Anatolia_N\", \"Steppe_MLBA\", \"WHG\", \n",
    "               \"I12433\", \"Levant_N\", \"Iran_N\"]\n",
    "\n",
    "base_folder = \"./output/qpAdm/distAlgIA.v46.3/\"\n",
    "\n",
    "### Get the IIDs\n",
    "df = pd.read_csv(\"./output/tables/qpadm.targets.distal.v46.3.tsv\", sep=\"\\t\") # CHANGE BACK TO ORIGNAL FILES\n",
    "iids0 = df[\"iid\"].values\n",
    "d = np.array([\"S18200.Y1.E2.L1\", 'MS10614.SG', \"I12433\"]) # Bad qpAdm runs?!\n",
    "iids= np.setdiff1d(iids0, d)\n",
    "print(f\"Loaded {len(iids)} IIDs for distal modelling\")\n",
    "\n",
    "df = get_df_n_way_model(ns=[1,2,3,4,5,6], comp_groups=comp_groups, targets=iids[:], \n",
    "                        base_folder=base_folder, reorder_sources=comp_groups) # 2,3,4,5,6\n",
    "\n",
    "### Save the Data to tabular Format\n",
    "df.to_csv(\"./output/qpAdm/v46.3/ind_model_distAlgIA.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the processed Data and play with some analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12096 qpAdm models.\n",
      "Feasable: 5199 models.\n",
      "Got 182 best Indivdiuals models\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./output/qpAdm/v46.3/ind_model_distAlgIA.tsv\", sep=\"\\t\")\n",
    "#df = pd.read_csv(\"./output/qpAdm/v46.3/ind_model_dist.tsv\", sep=\"\\t\")\n",
    "print(f\"Loaded {len(df)} qpAdm models.\")\n",
    "df1 = df[df[\"fs\"]==1].copy()\n",
    "print(f\"Feasable: {len(df1)} models.\")\n",
    "\n",
    "df_ind = get_individual_models(df1, min_p=0.01)\n",
    "print(f\"Got {len(df_ind)} best Indivdiuals models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"t\"]==\"VIL009\"].sort_values(by=\"p\", ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting at p>0.01\n",
      "\n",
      "Sources: 1\n",
      "('Anatolia_N',): 0\n",
      "('Steppe_MLBA',): 0\n",
      "('WHG',): 0\n",
      "('I12433',): 0\n",
      "('Iran_N',): 0\n",
      "('Levant_N',): 0\n",
      "\n",
      "Sources: 2\n",
      "('Anatolia_N', 'Steppe_MLBA'): 0\n",
      "('Anatolia_N', 'WHG'): 0\n",
      "('Anatolia_N', 'I12433'): 0\n",
      "('Anatolia_N', 'Iran_N'): 0\n",
      "('Anatolia_N', 'Levant_N'): 0\n",
      "('Steppe_MLBA', 'WHG'): 0\n",
      "('Steppe_MLBA', 'I12433'): 0\n",
      "('Steppe_MLBA', 'Iran_N'): 0\n",
      "('Steppe_MLBA', 'Levant_N'): 0\n",
      "('WHG', 'I12433'): 0\n",
      "('WHG', 'Iran_N'): 0\n",
      "('WHG', 'Levant_N'): 0\n",
      "('I12433', 'Iran_N'): 0\n",
      "('I12433', 'Levant_N'): 0\n",
      "('Iran_N', 'Levant_N'): 0\n",
      "\n",
      "Sources: 3\n",
      "('Anatolia_N', 'Steppe_MLBA', 'WHG'): 0\n",
      "('Anatolia_N', 'Steppe_MLBA', 'I12433'): 0\n",
      "('Anatolia_N', 'Steppe_MLBA', 'Iran_N'): 0\n",
      "('Anatolia_N', 'Steppe_MLBA', 'Levant_N'): 0\n",
      "('Anatolia_N', 'WHG', 'I12433'): 0\n",
      "('Anatolia_N', 'WHG', 'Iran_N'): 0\n",
      "('Anatolia_N', 'WHG', 'Levant_N'): 0\n",
      "('Anatolia_N', 'I12433', 'Iran_N'): 0\n",
      "('Anatolia_N', 'I12433', 'Levant_N'): 0\n",
      "('Anatolia_N', 'Iran_N', 'Levant_N'): 0\n",
      "('Steppe_MLBA', 'WHG', 'I12433'): 0\n",
      "('Steppe_MLBA', 'WHG', 'Iran_N'): 0\n",
      "('Steppe_MLBA', 'WHG', 'Levant_N'): 0\n",
      "('Steppe_MLBA', 'I12433', 'Iran_N'): 0\n",
      "('Steppe_MLBA', 'I12433', 'Levant_N'): 0\n",
      "('Steppe_MLBA', 'Iran_N', 'Levant_N'): 0\n",
      "('WHG', 'I12433', 'Iran_N'): 0\n",
      "('WHG', 'I12433', 'Levant_N'): 0\n",
      "('WHG', 'Iran_N', 'Levant_N'): 0\n",
      "('I12433', 'Iran_N', 'Levant_N'): 0\n",
      "\n",
      "Sources: 4\n",
      "('Anatolia_N', 'Steppe_MLBA', 'WHG', 'I12433'): 0\n",
      "('Anatolia_N', 'Steppe_MLBA', 'WHG', 'Iran_N'): 0\n",
      "('Anatolia_N', 'Steppe_MLBA', 'WHG', 'Levant_N'): 0\n",
      "('Anatolia_N', 'Steppe_MLBA', 'I12433', 'Iran_N'): 0\n",
      "('Anatolia_N', 'Steppe_MLBA', 'I12433', 'Levant_N'): 0\n",
      "('Anatolia_N', 'Steppe_MLBA', 'Iran_N', 'Levant_N'): 0\n",
      "('Anatolia_N', 'WHG', 'I12433', 'Iran_N'): 0\n",
      "('Anatolia_N', 'WHG', 'I12433', 'Levant_N'): 0\n",
      "('Anatolia_N', 'WHG', 'Iran_N', 'Levant_N'): 0\n",
      "('Anatolia_N', 'I12433', 'Iran_N', 'Levant_N'): 0\n",
      "('Steppe_MLBA', 'WHG', 'I12433', 'Iran_N'): 0\n",
      "('Steppe_MLBA', 'WHG', 'I12433', 'Levant_N'): 0\n",
      "('Steppe_MLBA', 'WHG', 'Iran_N', 'Levant_N'): 0\n",
      "('Steppe_MLBA', 'I12433', 'Iran_N', 'Levant_N'): 0\n",
      "('WHG', 'I12433', 'Iran_N', 'Levant_N'): 0\n",
      "\n",
      "Sources: 5\n",
      "('Anatolia_N', 'Steppe_MLBA', 'WHG', 'I12433', 'Iran_N'): 0\n",
      "('Anatolia_N', 'Steppe_MLBA', 'WHG', 'I12433', 'Levant_N'): 37\n",
      "('Anatolia_N', 'Steppe_MLBA', 'WHG', 'Iran_N', 'Levant_N'): 0\n",
      "('Anatolia_N', 'Steppe_MLBA', 'I12433', 'Iran_N', 'Levant_N'): 0\n",
      "('Anatolia_N', 'WHG', 'I12433', 'Iran_N', 'Levant_N'): 0\n",
      "('Steppe_MLBA', 'WHG', 'I12433', 'Iran_N', 'Levant_N'): 0\n",
      "\n",
      "Sources: 6\n",
      "('Anatolia_N', 'Steppe_MLBA', 'WHG', 'I12433', 'Iran_N', 'Levant_N'): 20\n"
     ]
    }
   ],
   "source": [
    "comp_groups = [\"Anatolia_N\", \"Steppe_MLBA\", \"WHG\", \n",
    "               \"I12433\", \"Iran_N\", \"Levant_N\"]\n",
    "p=0.01\n",
    "\n",
    "print(f\"Fitting at p>{p}\")\n",
    "for r in range(1,7):\n",
    "    print(f\"\\nSources: {r}\")\n",
    "    for ls in it.combinations(comp_groups, r=r):\n",
    "        df2 = get_spec_model(df1, sources=ls, output=False)   # \"Israel_Phoenician\"\n",
    "        s = np.sum(df2[\"p\"]>p)\n",
    "        print(f\"{ls}: {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv(\"/n/groups/reich/hringbauer/Data/v46.3.anno.csv\")\n",
    "dft = pd.merge(df1, df_meta[[\"iid\", \"loc\", \"clst\", \"n_cov_snp\"]], left_on=\"t\", right_on=\"iid\", how=\"left\")\n",
    "dft1 = get_individual_models(dft, min_p=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70 fitting rows.\n",
      "Fitting Models at p>0.01: 46\n"
     ]
    }
   ],
   "source": [
    "df2 = get_spec_model(dft, sources=['Anatolia_N', 'Steppe_MLBA', 'WHG'], tot_sources=6)   # \"Israel_Phoenician\"\n",
    "s = np.sum(df2[\"p\"]>0.01)\n",
    "print(f\"Fitting Models at p>0.01: {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft1[dft1[\"loc\"].str.contains(\"Poli\")][[\"p\", \"t\", \"clst\",  \"loc\", \"s1\", \"s2\", \"s3\", \"s4\", \"s5\", \"f1\", \"f2\", \"f3\", \"f4\",\n",
    "                                                   \"std1\", \"std2\", \"std3\", \"std4\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>p</th>\n",
       "      <th>n</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>std1</th>\n",
       "      <th>std2</th>\n",
       "      <th>std3</th>\n",
       "      <th>fs</th>\n",
       "      <th>iid</th>\n",
       "      <th>loc</th>\n",
       "      <th>clst</th>\n",
       "      <th>n_cov_snp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [t, p, n, s1, s2, s3, f1, f2, f3, std1, std2, std3, fs, iid, loc, clst, n_cov_snp]\n",
       "Index: []"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[df2[\"clst\"].str.contains(\"Iberia\")]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
