{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Eigenstrat files to run PCA with HO SNPs\n",
    "Extract and Merge in relevant populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute-e-16-229.o2.rc.hms.harvard.edu\n",
      "HSM Computational partition detected.\n",
      "/n/groups/reich/hringbauer/git/punic_aDNA\n",
      "CPU Count: 28\n",
      "3.7.4 (default, Sep 11 2019, 11:24:51) \n",
      "[GCC 6.2.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os  # For Saving to Folder\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import socket\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import itertools as it\n",
    "from time import time\n",
    "\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "\n",
    "if socket_name.startswith(\"compute-\"):\n",
    "    print(\"HSM Computational partition detected.\")\n",
    "    path = \"/n/groups/reich/hringbauer/git/punic_aDNA/\"  # The Path on Midway Cluster\n",
    "else:\n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "\n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "# Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(os.getcwd())\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_pops(df, string, col=\"clst\", \n",
    "                output=False):\n",
    "    \"\"\"Return list of clusters that contain string.\"\"\"\n",
    "    df1 = df[df[col].str.contains(string)]\n",
    "    if output:\n",
    "        print(df1[col].value_counts())\n",
    "    clsts = list(set(df1[col].values))\n",
    "    print(f\"Found #clsts labels containing {string}: {len(clsts)}\")\n",
    "\n",
    "    return clsts\n",
    "\n",
    "def run_convertf(path_convertf = \"./o2bin/convertf\", parfile = \"./parfiles/convertf.keep.par\"):\n",
    "    \"\"\"Runs the Downsampling\"\"\"\n",
    "    ! $path_convertf -p $parfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the .ind File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 37234 Individuals\n"
     ]
    }
   ],
   "source": [
    "vrs = \"49.2\"\n",
    "v0 = vrs.split(\".\")[0]\n",
    "base_path = f\"/n/groups/reich/DAVID/V{v0}/V{vrs}/v{vrs}_HO\"\n",
    "\n",
    "ind_path = base_path + \".ind\"\n",
    "\n",
    "df_ind = pd.read_csv(ind_path, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "print(f\"Loaded {len(df_ind)} Individuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definie what target populations to pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ancients\n",
    "Make sure all cluster labels have at least one match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found #clsts labels containing Algeria: 5\n",
      "Found #clsts labels containing Morocco: 8\n",
      "Found #clsts labels containing Tunisia: 12\n",
      "Found #clsts labels containing Punic: 42\n",
      "Found #clsts labels containing Phoenician: 3\n",
      "Found #clsts labels containing Spain_Vandal: 5\n",
      "Found #clsts labels containing Spain_LBA: 7\n",
      "Found #clsts labels containing Spain_Punic: 18\n",
      "Found #clsts labels containing Sardinia: 78\n",
      "Found #clsts labels containing Ibiza: 1\n",
      "Found #clsts labels containing Israel_MLBA: 14\n",
      "Found #clsts labels containing Israel_LBA: 4\n",
      "Found #clsts labels containing Israel_IA: 3\n",
      "Found #clsts labels containing Israel_LIA: 1\n",
      "Found #clsts labels containing Ashkelon: 4\n",
      "Found #clsts labels containing Sicily: 82\n",
      "Found #clsts labels containing Hellenistic: 33\n",
      "Found #clsts labels containing Israel_IA: 3\n",
      "Found #clsts labels containing Israel_EIA: 1\n",
      "Found #clsts labels containing Israel_Persian: 1\n",
      "Found #clsts labels containing Gibraltar: 2\n",
      "Found #clsts labels containing Lebanon: 15\n",
      "Found #clsts labels containing Spain_EBA_Africa: 3\n",
      "Found #clsts labels containing Spain_BellBeaker_oAfrica: 2\n",
      "Found #clsts labels containing Spain_Greek: 3\n",
      "Found #clsts labels containing Spain_Hellenistic: 4\n",
      "Found #clsts labels containing Spain_IA: 18\n",
      "Found #clsts labels containing Italy_Sardinia_C_oAfrica: 2\n",
      "Found #clsts labels containing Nigeria_IA: 1\n",
      "Found #clsts labels containing Nigeria_Medieval: 2\n",
      "Found #clsts labels containing Mallorca: 1\n",
      "Found #clsts labels containing Menorca: 4\n",
      "Found #clsts labels containing Egypt_Hellenistic: 5\n",
      "Found #clsts labels containing Egypt_Roman: 3\n",
      "Found #clsts labels containing Egypt_Dynastic: 5\n",
      "Found #clsts labels containing Egypt_Third: 3\n",
      "Found #clsts labels containing Spain_Roman_oAfrica2: 1\n",
      "Found #clsts labels containing Greece_: 54\n",
      "Found #clsts labels containing Guanche: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "454"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pops = [\"Algeria\", \"Morocco\", \"Tunisia\", \"Punic\", \"Phoenician\", \"Spain_Vandal\", \"Spain_LBA\",\n",
    "        \"Spain_Punic\", \"Sardinia\", \"Ibiza\", \"Israel_MLBA\", \"Israel_LBA\", \"Israel_IA\", \"Israel_LIA\", \n",
    "        \"Ashkelon\", \"Sicily\", \"Hellenistic\",\n",
    "        \"Israel_IA\", \"Israel_EIA\", \"Israel_Persian\", \"Gibraltar\", \"Lebanon\",\n",
    "        \"Spain_EBA_Africa\", \"Spain_BellBeaker_oAfrica\", \"Spain_Greek\",\n",
    "        \"Spain_Hellenistic\", \"Spain_IA\", \"Italy_Sardinia_C_oAfrica\", \n",
    "        \"Nigeria_IA\", \"Nigeria_Medieval\", \"Mallorca\", \"Menorca\", \n",
    "        \"Egypt_Hellenistic\", \"Egypt_Roman\", \"Egypt_Dynastic\", \"Egypt_Third\",\n",
    "        \"Spain_Roman_oAfrica2\",\n",
    "        \"Greece_\", \"Guanche\"]\n",
    "\n",
    "clsts = [return_pops(df_ind, string=pop, \n",
    "                     output=False) for pop in pops]\n",
    "\n",
    "clsts = [inner for ls in clsts for inner in ls]\n",
    "len(clsts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moderns\n",
    "Get list of Human Origin Populations to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1196 Individuals\n"
     ]
    }
   ],
   "source": [
    "path_ho = \"/n/groups/reich/hringbauer/git/punic_aDNA/parfiles/pca/construct_WE_NA_PCA.v48.2.list\" # Changed some HO labels \n",
    "\n",
    "df_ho = pd.read_csv(path_ho, header=None, sep=r\"\\s+\", engine=\"python\")\n",
    "df_ho.columns=[\"iid\", \"pop\"]\n",
    "print(f\"Loaded {len(df_ho)} Individuals\")\n",
    "\n",
    "pops = set(df_ho[\"pop\"])\n",
    "clsts1 = [p.rsplit(\"_\", 1)[0] for p in pops]\n",
    "l = [np.sum(df_ind[\"clst\"].str.contains(p)) for p in clsts1]\n",
    "assert(np.min(l)>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and save final pop list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 456 Populations\n",
      "After Exclusion 384 populations\n"
     ]
    }
   ],
   "source": [
    "exclude_strings = [\"_lc\", \"contam\"] # \"_d\"\n",
    "\n",
    "clsts = list(set(clsts).union(set(clsts1))) # Filter to unique Elements\n",
    "print(f\"Loaded {len(clsts)} Populations\")\n",
    "\n",
    "### Exclude Strings\n",
    "for ex in exclude_strings:\n",
    "    clsts = [c for c in clsts if ex not in c]\n",
    "print(f\"After Exclusion {len(clsts)} populations\")\n",
    "clsts = clsts + [\"include\"]\n",
    "### Originally Loaded 379 Populations\n",
    "# After Exclusion 289 populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 385 population names to ./parfiles/pca/keep_pops.v49.2\n"
     ]
    }
   ],
   "source": [
    "keep = np.array(clsts)\n",
    "path_keep = f\"./parfiles/pca/keep_pops.v{vrs}\" # keep_pops for Kerkouane\n",
    "np.savetxt(path_keep, keep, fmt=\"%s\")\n",
    "print(f\"Saved {len(keep)} population names to {path_keep}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create .ind file with flagged out pop names\n",
    "Idea: Some individuals should not be included in the final .ind file. To do this,\n",
    "I create a .ind file where the population of these is set to \"Ignore1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flagged out 752/37234 downsampled Individuals\n",
      "Saved to: /n/groups/reich/hringbauer/Data/v49.2.flagged.ind\n"
     ]
    }
   ],
   "source": [
    "base_path = f\"/n/groups/reich/DAVID/V{v0}/V{vrs}/v{vrs}_HO\"\n",
    "save_path = f\"/n/groups/reich/hringbauer/Data/v{vrs}.flagged.ind\"\n",
    "\n",
    "ind_path = base_path + \".ind\"\n",
    "\n",
    "df_ind = pd.read_csv(ind_path, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "\n",
    "idx = df_ind[\"iid\"].str.endswith(\"_d\")\n",
    "df_ind.loc[idx, \"clst\"] = \"Ignore1\"\n",
    "print(f\"Flagged out {np.sum(idx)}/{len(idx)} downsampled Individuals\")\n",
    "df_ind.to_csv(save_path, header=False, sep=\" \", index=False)\n",
    "print(f\"Saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include Individuals from Ilan's List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Including 147/148 IIDs from external source\n",
      "Saved to: /n/groups/reich/hringbauer/Data/v49.2.flagged.included.ind\n"
     ]
    }
   ],
   "source": [
    "save_path2 = f\"/n/groups/reich/hringbauer/Data/v{vrs}.flagged.included.ind\"\n",
    "\n",
    "df_add = pd.read_csv(\"./data/v49-added-samples.txt\", header=None, sep=r\"\\s+\", engine=\"python\")\n",
    "df_add.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "df_ind = pd.read_csv(save_path, delim_whitespace=True, header=None)\n",
    "df_ind.columns=[\"iid\", \"sex\", \"clst\"]\n",
    "\n",
    "### Add the additional Indivudals\n",
    "add_inds = [\"RISE507.508.merge.SG\", \"I13517_d\", \"I13518_d\", \"I13519_d\"] # Renamed indivdual plus some Myceneans\n",
    "search_inds = np.concatenate((df_add[\"iid\"], add_inds))\n",
    "\n",
    "idx = df_ind[\"iid\"].isin(search_inds)\n",
    "print(f\"Including {np.sum(idx)}/{len(search_inds)} IIDs from external source\")\n",
    "\n",
    "df_ind.loc[idx, \"clst\"] = \"include\"\n",
    "df_ind.to_csv(save_path2, header=False, sep=\" \", index=False)\n",
    "print(f\"Saved to: {save_path2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run convertf\n",
    "Takes abouend 20 min for all individuals\n",
    "\n",
    "Change all required additional parameters in manually encoded parfile!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter file: ./parfiles/pca/convertf.keep.v49.2.par\n",
      "BASE: /n/groups/reich/\n",
      "DIR: DAVID/V49/V49.2/v49.2_HO\n",
      "OUT: hringbauer/git/punic_aDNA/eigenstrat/punic.v49.2_HO\n",
      "genotypename: /n/groups/reich//DAVID/V49/V49.2/v49.2_HO.geno\n",
      "snpname: /n/groups/reich//DAVID/V49/V49.2/v49.2_HO.snp\n",
      "indivname: /n/groups/reich/hringbauer/Data/v49.2.flagged.included.ind\n",
      "genooutfilename: /n/groups/reich//hringbauer/git/punic_aDNA/eigenstrat/punic.v49.2_HO.geno\n",
      "snpoutfilename: /n/groups/reich//hringbauer/git/punic_aDNA/eigenstrat/punic.v49.2_HO.snp\n",
      "indoutfilename: /n/groups/reich//hringbauer/git/punic_aDNA/eigenstrat/punic.v49.2_HO.ind\n",
      "outputformat: PACKEDANCESTRYMAP\n",
      "hashcheck: YES\n",
      "poplistname: /n/groups/reich//hringbauer/git/punic_aDNA/parfiles/pca/keep_pops.v49.2\n",
      "## /n/groups/reich/hringbauer/o2bin/convertf version: 5750\n",
      "read 1073741824 bytes\n",
      "read 2147483648 bytes\n",
      "read 3221225472 bytes\n",
      "read 4294967296 bytes\n",
      "read 5368709120 bytes\n",
      "read 5562807057 bytes\n",
      "packed geno read OK\n",
      "end of inpack\n",
      "before compress: snps: 597573 indivs: 37234\n",
      "after compress: snps: 597573 indivs: 2373\n",
      "numvalidind:   2373  maxmiss: 2373001\n",
      "packedancestrymap output\n",
      "numsnps output: 597573\n",
      "##end of convertf:      576.560 seconds cpu      368.038 Mbytes in use\n",
      "CPU times: user 14.2 s, sys: 2.98 s, total: 17.2 s\n",
      "Wall time: 9min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_convertf(path_convertf = \"/n/groups/reich/hringbauer/o2bin/convertf\", \n",
    "             parfile = f\"./parfiles/pca/convertf.keep.v{vrs}.par\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the .ind file to have one population to project on in moderns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1187/2373 HO individuals\n",
      "Saved 2373 overall individuals to /n/groups/reich/hringbauer/git/punic_aDNA/eigenstrat/punic.v49.2_HO.pca.ind\n",
      "Found 1187/2373 of Alissas _mod pops\n"
     ]
    }
   ],
   "source": [
    "path_ind = f\"/n/groups/reich/hringbauer/git/punic_aDNA/eigenstrat/punic.v{vrs}_HO.ind\"\n",
    "path_mod = f\"/n/groups/reich/hringbauer/git/punic_aDNA/eigenstrat/punic.v{vrs}_HO.pca.ind\"\n",
    "\n",
    "df_ind = pd.read_csv(path_ind, header=None, sep=r\"\\s+\", engine=\"python\")\n",
    "df_ind.columns = [\"iid\", \"sex\", \"pop\"]\n",
    "iids = df_ho[\"iid\"].values # Alissas original IIDs\n",
    "idx = [iid in iids for iid in df_ind[\"iid\"]]\n",
    "print(f\"Found {np.sum(idx)}/{len(idx)} HO individuals\")\n",
    "\n",
    "df_ind.loc[idx, \"pop\"]  = \"construct_WE_NA_PCA\" #df_ind.loc[idx, \"pop\"] + \"_mod\" \n",
    "df_ind.to_csv(path_mod, sep=\" \", index=None, header=False)\n",
    "print(f\"Saved {len(df_ind)} overall individuals to {path_mod}\")\n",
    "\n",
    "### Sanity Check \n",
    "#idx = [p in pops for p in df_ind[\"pop\"]] \n",
    "idx = [(p==\"construct_WE_NA_PCA\") for p in df_ind[\"pop\"]] \n",
    "print(f\"Found {np.sum(idx)}/{len(idx)} of Alissas _mod pops\")\n",
    "# in v45: 1196/2169 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1187/1196 of Alissas _mod pops\n"
     ]
    }
   ],
   "source": [
    "### Needed only for trouble shooting ###\n",
    "found = [iid in df_ind[\"iid\"].values for iid in df_ho[\"iid\"].values]\n",
    "print(f\"Found {np.sum(found)}/{len(found)} of Alissas _mod pops\")\n",
    "#df_ho[~np.array(found)][\"pop\"].value_counts() # Only for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>HGDP00741</td>\n",
       "      <td>Palestinian_mod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>abh107</td>\n",
       "      <td>Abkhasian_mod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>NorthOssetia5</td>\n",
       "      <td>Ossetian_mod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>NorthOssetia12</td>\n",
       "      <td>Ossetian_mod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>HG00181</td>\n",
       "      <td>Finnish_mod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>HG01695</td>\n",
       "      <td>IBS_CanaryIslands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>HG01694</td>\n",
       "      <td>IBS_CanaryIslands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>IL4</td>\n",
       "      <td>Greek_mod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>ROS005</td>\n",
       "      <td>Spanish_mod</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 iid                pop\n",
       "242        HGDP00741    Palestinian_mod\n",
       "450           abh107      Abkhasian_mod\n",
       "466    NorthOssetia5       Ossetian_mod\n",
       "485   NorthOssetia12       Ossetian_mod\n",
       "598          HG00181        Finnish_mod\n",
       "616          HG01695  IBS_CanaryIslands\n",
       "617          HG01694  IBS_CanaryIslands\n",
       "951              IL4          Greek_mod\n",
       "1159          ROS005        Spanish_mod"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ho[~np.array(found)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And now sbatch the PCA script.\n",
    "Takes about 9h for 1000 extra samples\n",
    "\n",
    "Manually do it in `./parfiles/pca/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Meta File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv(\"/n/groups/reich/hringbauer/Data/v46.3.anno.csv\")\n",
    "path_ho = \"/n/groups/reich//hringbauer/git/punic_aDNA/eigenstrat/combined/punic.v46.3.share.ind\"\n",
    "df_ho = pd.read_csv(path_ho, header=None, sep=r\"\\s+\", engine=\"python\")\n",
    "df_ho.columns = [\"iid\", \"sex\", \"clst\"]\n",
    "\n",
    "df_save = pd.merge(df_ho[\"iid\"], df_meta, on=\"iid\")\n",
    "df_save = df_save.sort_values(by=\"clst\")\n",
    "#df_save.to_csv(\"./data/meta/v46.3_punic_meta.tsv\", sep=\"\\t\", index=False)\n",
    "df_save.to_csv(\"./output/share/v46.3_punic_meta.share.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1194"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
