{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate F_ST Matrix for Punic project\n",
    "This is run on v46.3 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os  # For Saving to Folder\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import socket\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import itertools as it\n",
    "from time import time\n",
    "\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "\n",
    "if socket_name.startswith(\"compute-\"):\n",
    "    print(\"HSM Computational partition detected.\")\n",
    "    path = \"/n/groups/reich/hringbauer/git/punic_aDNA/\"  # The Path on Midway Cluster\n",
    "else:\n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "\n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "# Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(os.getcwd())\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meta_df(f, path_meta=\"/n/groups/reich/hringbauer/Data/v43.4.anno.csv\"):\n",
    "    \"\"\"Create and return Meta Dataframe that matches hdf5 in format\"\"\"\n",
    "    samples = f[\"samples\"][:].astype(\"str\")\n",
    "    df_h5 = pd.DataFrame({\"iid\":samples})\n",
    "    df_meta = pd.read_csv(path_meta)\n",
    "    print(f\"Loaded {len(df_meta)} Samples\")\n",
    "    df = pd.merge(df_h5, df_meta, on=\"iid\", how=\"left\")\n",
    "    print(f\"Created matching Meta Dataframe for h5: {len(df)}\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    df[\"clst\"] = df[\"clst\"].fillna(\"Not Available\")\n",
    "    return df\n",
    "\n",
    "def get_cluster_idx(df, clst=\"\", col_clst=\"clst\", \n",
    "                    age_range=[], exact=False,\n",
    "                    include_col=\"include\"):\n",
    "    \"\"\"Get idcs of all samples within Cluster\n",
    "    If age_range, only samples in age range\"\"\"\n",
    "    if len(include_col)>0:\n",
    "        idcs1 = df[include_col]==True\n",
    "    else:\n",
    "        idcs1 = True\n",
    "        \n",
    "    if exact:\n",
    "        idcs = np.where((df[col_clst]==clst) & idcs1)[0] \n",
    "    else:\n",
    "        idcs = np.where((df[col_clst].str.contains(clst)) & idcs1)[0]\n",
    "    \n",
    "    ### Do additional Filtering\n",
    "    if len(age_range)>0:  \n",
    "        pass\n",
    "    return idcs\n",
    "\n",
    "###############################\n",
    "### Calculate the Allele Counts\n",
    "def get_ph(f, idcs):\n",
    "    \"\"\"Sample pseudohaploid data for hdf5 for\n",
    "    individuals with indices idcs\"\"\"\n",
    "    ads = f[\"calldata/AD\"][:,idcs,:2]\n",
    "    ads[ads<0]=0 # Set nmissing data to 0\n",
    "    cov = np.sum(ads, axis=2) # get the coverage per locus/indiviual\n",
    "    idx = cov>0  # Where there is some coverage\n",
    "    p = np.divide(ads[:,:,1], cov, where=idx)\n",
    "    p[~idx]=1\n",
    "    p = np.clip(p, a_min=0, a_max=1) # Santity check to deal with numerics\n",
    "    ac = np.random.binomial(1,p)\n",
    "    ac[~idx] = -1\n",
    "    return ac\n",
    "\n",
    "def get_gt(f, idcs):\n",
    "    \"\"\"Get diploid genoytpe counts\"\"\"\n",
    "    gt = f[\"calldata/GT\"][:,idcs,:]\n",
    "    assert(np.min(gt)>=0)\n",
    "    gt = np.sum(gt, axis=2) # Count #derived variants\n",
    "    return gt\n",
    "\n",
    "def calc_ac_from_ph(ph):\n",
    "    \"\"\"Calculate allele allele counts for individuals \n",
    "    with indices from hdf5 with allele counts only\n",
    "    ph: Array of pseudo-haploid [l,n]\"\"\"\n",
    "    c_ref=np.sum(ph==0, axis=1) # Sum the Counts over all Individuals\n",
    "    c_alt=np.sum(ph==1, axis=1) # Sum the Counts over all Individuals\n",
    "    \n",
    "    # Double 0,0 no problem, goes to NaN and is then caught by allel\n",
    "    return np.column_stack((c_ref, c_alt)) # Return the nx2 Allele Counts\n",
    "\n",
    "def calc_ac_from_gt(gt):\n",
    "    \"\"\"Calculate allele allele counts for individuals \n",
    "    with indices from hdf5 with allele counts only\n",
    "    ph: Array of pseudo-haploid [l,n]\"\"\"\n",
    "    c_ref= 2*np.sum(gt==0, axis=1) + np.sum(gt==1, axis=1)\n",
    "    c_alt= 2*np.sum(gt==2, axis=1) + np.sum(gt==1, axis=1)\n",
    "    return np.column_stack((c_ref, c_alt)) # Return the nx2 Allele Counts\n",
    "\n",
    "def get_ac_from_f(f, idcs, ph=True):\n",
    "    \"\"\"Get Allele Counts from HDF, \n",
    "    grouped for all indivdiuals in idcs\n",
    "    ph: Whether to use pseudo-haploid or diploid genotypes\"\"\"\n",
    "    if ph:\n",
    "        ph = get_ph(f, idcs)\n",
    "        ac = calc_ac_from_ph(ph=ph)\n",
    "    else:\n",
    "        print(\"Using diploid mode...\")\n",
    "        gt = get_gt(f, idcs)\n",
    "        ac = calc_ac_from_gt(gt)\n",
    "    return ac\n",
    "\n",
    "def calculate_ac_pop(clst, f, df, col=\"clst\", exact=False, \n",
    "                     ph=True, include_col=\"include\"):\n",
    "    \"\"\"Return allele counts for population.\n",
    "    exact: whether ther is an exact match\"\"\"\n",
    "    idcs = get_cluster_idx(df, clst=clst, exact=exact,\n",
    "                           col_clst=col, include_col=include_col)\n",
    "    ac = get_ac_from_f(f, idcs, ph=ph) \n",
    "    return ac\n",
    "\n",
    "def calculate_ac_pops(pops, f, df, col=\"clst\", ph=True, \n",
    "                      exact=False, out=True):\n",
    "    \"\"\"Calculate list of allele counts [l,2] for pops\n",
    "    f: hdf5\n",
    "    df: metafile matching f\n",
    "    pops: List of populations to extract ACs for\"\"\"\n",
    "    ### Check whether all pops have matches first:\n",
    "    idcss = [get_cluster_idx(df=df, clst=pop, exact=exact, col_clst=col)\n",
    "                              for pop in pops]\n",
    "    counts = np.array(map(len, idcss))\n",
    "    if np.min(counts)==0:\n",
    "        idx = np.where(counts==0)[0]\n",
    "        raise RuntimeError(f\"Pops {pop[idx]} not found!\")\n",
    "    \n",
    "    acs=[]\n",
    "    for pop in pops:\n",
    "        idcs = get_cluster_idx(df=df, clst=pop, exact=exact, col_clst=col)\n",
    "        if len(idcs)==0:\n",
    "            raise RuntimeWarning(f\"No matching iids for {pop} not found!!\")\n",
    "        if out:\n",
    "            print(f\"Calculating counts pop: {pop}, n={len(idcs)}...\")\n",
    "        ac = get_ac_from_f(f, idcs, ph=ph) \n",
    "        acs.append(ac)\n",
    "    return acs\n",
    "\n",
    "###########################################\n",
    "###########################################\n",
    "### Calculate the actual f statistics\n",
    "\n",
    "def f3_ac(pt, p1, p2, snps_okay=None, blen=1000):\n",
    "    \"\"\"Calculate f3 for Allele Counts (lx2 arrays)\n",
    "    snps_okay: Which SNPs to actually use. If none use all\n",
    "    blen: Block Nr for Bootstrap\n",
    "    \"\"\"\n",
    "    #f3 = np.mean((pt-p1)*(pt-p2))\n",
    "    f3 = allel.average_patterson_f3(pt, p1, p2, blen=blen, normed=False)\n",
    "    return [f3[0], f3[1], f3[2]]  # f4, se, z\n",
    "\n",
    "def f4_ac(p1, p2, p3, p4, snps_okay=None, blen=1000):\n",
    "    \"\"\"Calculate f4 for Allele Counts (lx2 arrays)\n",
    "    snps_okay: Which SNPs to actually use (If none use all)\n",
    "    blen: Block Nr for Bootstrap\n",
    "    \"\"\"\n",
    "    f4 = allel.average_patterson_d(p1, p2, p3, p4, blen=blen)\n",
    "    return [f4[0], f4[1], f4[2]]  # f4, se, z\n",
    "\n",
    "def fst_ac(p1, p2, blen=1000):\n",
    "    \"\"\"Calculate f3 for Allele Counts (lx2 arrays)\n",
    "    blen: Block Nr for Bootstrap\n",
    "    A sim wrapper, so later on different methods can be implemented.\n",
    "    Return fst, se, z value (based on jackkniving)\n",
    "    \"\"\"\n",
    "    res = allel.average_patterson_fst(p1, p2, blen=blen)\n",
    "    f4, se = res[0], res[1]\n",
    "    z = f4 / se # Calculate the z-Value\n",
    "    return [res[0], res[1], z]  # f4, se, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path_anno = \"./data/v46.4.anno.csv\"\n",
    "path_h5 = \"/n/groups/reich/hringbauer/git/hapBLOCK/data/hdf5/1240k_v46.2/all_ch.h5\"\n",
    "path_meta = \"./data/meta_v1.tsv\"\n",
    "\n",
    "f = h5py.File(path_h5, \"r\")\n",
    "df = create_meta_df(f, path_meta=path_anno)\n",
    "\n",
    "### Merge in the cluster labels\n",
    "df1 = pd.read_csv(path_meta, sep=\"\\t\")\n",
    "### Only include unique,unrelated samples\n",
    "df1 = df1[df1[\"include\"]==1].copy().reset_index(drop=True) \n",
    "df1 = pd.merge(df,df1[[\"iid\", \"label_region\", \"include\"]], on=\"iid\", how=\"left\")\n",
    "df1.loc[df1[\"label_region\"].isnull(), \"label_region\"]=\"not assigned\"\n",
    "assert(len(df1)==len(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
