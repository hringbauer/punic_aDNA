{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to calculate F_ST Matrix for Punic project\n",
    "This is run on v54.1 data from the HDF5.\n",
    "\n",
    "Development Area. Will be packed into functions for\n",
    "calculation and plotting notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute-e-16-236.o2.rc.hms.harvard.edu\n",
      "HSM Computational partition detected.\n",
      "/n/groups/reich/hringbauer/git/punic_aDNA\n",
      "CPU Count: 28\n",
      "3.7.4 (default, Sep 11 2019, 11:24:51) \n",
      "[GCC 6.2.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os  # For Saving to Folder\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import socket\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import itertools as it\n",
    "from time import time\n",
    "import h5py as h5py\n",
    "import allel ### To Calculate Scikit Allel\n",
    "\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "\n",
    "if socket_name.startswith(\"compute-\"):\n",
    "    print(\"HSM Computational partition detected.\")\n",
    "    path = \"/n/groups/reich/hringbauer/git/punic_aDNA/\"  # The Path on Midway Cluster\n",
    "else:\n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "\n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "# Show the current working directory. Should be HAPSBURG/Notebooks/ParallelRuns\n",
    "print(os.getcwd())\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meta_df(f, path_meta=\"/n/groups/reich/hringbauer/Data/v49.2.anno.csv\"):\n",
    "    \"\"\"Create and return Meta Dataframe that matches hdf5 in format\"\"\"\n",
    "    samples = f[\"samples\"][:].astype(\"str\")\n",
    "    df_h5 = pd.DataFrame({\"iid\":samples})\n",
    "    df_meta = pd.read_csv(path_meta)\n",
    "    print(f\"Loaded {len(df_meta)} rows from Metafile\")\n",
    "    df = pd.merge(df_h5, df_meta, on=\"iid\", how=\"left\")\n",
    "    print(f\"Created matching Meta Dataframe for h5: {len(df)}\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    df[\"clst\"] = df[\"clst\"].fillna(\"Not Available\")\n",
    "    return df\n",
    "\n",
    "def set_clst(df, idx_list=[], clst_new=\"\", \n",
    "             col_iid=\"iid\", col_clst=\"clst\"):\n",
    "    \"\"\"Set New Cluster Label to Dataframe\"\"\"\n",
    "    idx = df[col_iid].isin(idx_list)\n",
    "    df.loc[idx, col_clst] = clst_new\n",
    "    print(f\"Updated {np.sum(idx)} Individuals {col_clst}: {clst_new}\")\n",
    "\n",
    "def get_cluster_idx(df, clst=\"\", col_clst=\"clst\", \n",
    "                    age_range=[], exact=False,\n",
    "                    include_col=\"include\"):\n",
    "    \"\"\"Get idcs of all samples within Cluster\n",
    "    If age_range, only samples in age range\"\"\"\n",
    "    if len(include_col)>0:\n",
    "        idcs1 = df[include_col]==True\n",
    "    else:\n",
    "        idcs1 = True\n",
    "        \n",
    "    if exact:\n",
    "        idcs = np.where((df[col_clst]==clst) & idcs1)[0] \n",
    "    else:\n",
    "        idcs = np.where((df[col_clst].str.contains(clst)) & idcs1)[0]\n",
    "    \n",
    "    ### Do additional Filtering\n",
    "    if len(age_range)>0:  \n",
    "        pass\n",
    "    return idcs\n",
    "\n",
    "###############################\n",
    "### Calculate the Allele Counts\n",
    "def get_ph(f, idcs):\n",
    "    \"\"\"Sample pseudohaploid data for hdf5 for\n",
    "    individuals with indices idcs\"\"\"\n",
    "    ads = f[\"calldata/AD\"][:,idcs,:2]\n",
    "    ads[ads<0]=0 # Set nmissing data to 0\n",
    "    cov = np.sum(ads, axis=2) # get the coverage per locus/indiviual\n",
    "    idx = cov>0  # Where there is some coverage\n",
    "    p = np.divide(ads[:,:,1], cov, where=idx)\n",
    "    p[~idx]=1\n",
    "    p = np.clip(p, a_min=0, a_max=1) # Santity check to deal with numerics\n",
    "    ac = np.random.binomial(1,p)\n",
    "    ac[~idx] = -1\n",
    "    return ac\n",
    "\n",
    "def get_gt(f, idcs):\n",
    "    \"\"\"Get diploid genoytpe counts\"\"\"\n",
    "    gt = f[\"calldata/GT\"][:,idcs,:]\n",
    "    assert(np.min(gt)>=0)\n",
    "    gt = np.sum(gt, axis=2) # Count #derived variants\n",
    "    return gt\n",
    "\n",
    "def calc_ac_from_ph(ph):\n",
    "    \"\"\"Calculate allele allele counts for individuals \n",
    "    with indices from hdf5 with allele counts only\n",
    "    ph: Array of pseudo-haploid [l,n]\"\"\"\n",
    "    c_ref=np.sum(ph==0, axis=1) # Sum the Counts over all Individuals\n",
    "    c_alt=np.sum(ph==1, axis=1) # Sum the Counts over all Individuals\n",
    "    \n",
    "    # Double 0,0 no problem, goes to NaN and is then caught by allel\n",
    "    return np.column_stack((c_ref, c_alt)) # Return the nx2 Allele Counts\n",
    "\n",
    "def calc_ac_from_gt(gt):\n",
    "    \"\"\"Calculate allele allele counts for individuals \n",
    "    with indices from hdf5 with allele counts only\n",
    "    ph: Array of pseudo-haploid [l,n]\"\"\"\n",
    "    c_ref= 2*np.sum(gt==0, axis=1) + np.sum(gt==1, axis=1)\n",
    "    c_alt= 2*np.sum(gt==2, axis=1) + np.sum(gt==1, axis=1)\n",
    "    return np.column_stack((c_ref, c_alt)) # Return the nx2 Allele Counts\n",
    "\n",
    "def get_ac_from_f(f, idcs, ph=True):\n",
    "    \"\"\"Get Allele Counts from HDF, \n",
    "    grouped for all indivdiuals in idcs\n",
    "    ph: Whether to use pseudo-haploid or diploid genotypes\"\"\"\n",
    "    if ph:\n",
    "        ph = get_ph(f, idcs)\n",
    "        ac = calc_ac_from_ph(ph=ph)\n",
    "    else:\n",
    "        print(\"Using diploid mode...\")\n",
    "        gt = get_gt(f, idcs)\n",
    "        ac = calc_ac_from_gt(gt)\n",
    "    return ac\n",
    "\n",
    "def calculate_ac_pop(clst, f, df, col=\"clst\", exact=False, \n",
    "                     ph=True, include_col=\"include\"):\n",
    "    \"\"\"Return allele counts for population.\n",
    "    exact: whether ther is an exact match\"\"\"\n",
    "    idcs = get_cluster_idx(df, clst=clst, exact=exact,\n",
    "                           col_clst=col, include_col=include_col)\n",
    "    ac = get_ac_from_f(f, idcs, ph=ph) \n",
    "    return ac\n",
    "\n",
    "def calculate_ac_pops(pops, f, df, col=\"clst\", ph=True, \n",
    "                      exact=False, out=True):\n",
    "    \"\"\"Calculate list of allele counts [l,2] for pops\n",
    "    f: hdf5\n",
    "    df: metafile matching f\n",
    "    pops: List of populations to extract ACs for\"\"\"\n",
    "    ### Check whether all pops have matches first:\n",
    "    idcss = [get_cluster_idx(df=df, clst=pop, exact=exact, col_clst=col)\n",
    "                              for pop in pops]\n",
    "    counts = np.array(map(len, idcss))\n",
    "    if np.min(counts)==0:\n",
    "        idx = np.where(counts==0)[0]\n",
    "        raise RuntimeError(f\"Pops {pop[idx]} not found!\")\n",
    "    \n",
    "    acs=[]\n",
    "    for pop in pops:\n",
    "        idcs = get_cluster_idx(df=df, clst=pop, exact=exact, col_clst=col)\n",
    "        if len(idcs)==0:\n",
    "            raise RuntimeWarning(f\"No matching iids for {pop} not found!!\")\n",
    "        if out:\n",
    "            print(f\"Calculating counts pop: {pop}, n={len(idcs)}...\")\n",
    "        ac = get_ac_from_f(f, idcs, ph=ph) \n",
    "        acs.append(ac)\n",
    "    return acs\n",
    "\n",
    "###########################################\n",
    "###########################################\n",
    "### Calculate the actual f statistics\n",
    "\n",
    "def f3_ac(pt, p1, p2, snps_okay=None, blen=1000):\n",
    "    \"\"\"Calculate f3 for Allele Counts (lx2 arrays)\n",
    "    snps_okay: Which SNPs to actually use. If none use all\n",
    "    blen: Block Nr for Bootstrap\n",
    "    \"\"\"\n",
    "    #f3 = np.mean((pt-p1)*(pt-p2))\n",
    "    f3 = allel.average_patterson_f3(pt, p1, p2, blen=blen, normed=False)\n",
    "    return [f3[0], f3[1], f3[2]]  # f4, se, z\n",
    "\n",
    "def f4_ac(p1, p2, p3, p4, snps_okay=None, blen=1000):\n",
    "    \"\"\"Calculate f4 for Allele Counts (lx2 arrays)\n",
    "    snps_okay: Which SNPs to actually use (If none use all)\n",
    "    blen: Block Nr for Bootstrap\n",
    "    \"\"\"\n",
    "    f4 = allel.average_patterson_d(p1, p2, p3, p4, blen=blen)\n",
    "    return [f4[0], f4[1], f4[2]]  # f4, se, z\n",
    "\n",
    "def fst_ac(p1, p2, blen=1000):\n",
    "    \"\"\"Calculate f3 for Allele Counts (lx2 arrays)\n",
    "    blen: Block Nr for Bootstrap\n",
    "    A sim wrapper, so later on different methods can be implemented.\n",
    "    Return fst, se, z value (based on jackkniving)\n",
    "    \"\"\"\n",
    "    res = allel.average_patterson_fst(p1, p2, blen=blen)\n",
    "    f4, se = res[0], res[1]\n",
    "    z = f4 / se # Calculate the z-Value\n",
    "    return [res[0], res[1], z]  # f4, se, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path_anno = \"/n/groups/reich/hringbauer/Data/v54.1.anno.csv\"\n",
    "path_h5 = \"/n/groups/reich/hringbauer/git/hapBLOCK/data/hdf5/1240k_v56.3/XXX.h5\"\n",
    "#path_meta = \"/n/groups/reich/hringbauer/Data/\"/v49.2_punic_meta.tsv\"\n",
    "\n",
    "f = h5py.File(path_h5, \"r\")\n",
    "df = create_meta_df(f, path_meta=path_anno)\n",
    "\n",
    "### Get only the highest coverage individuals\n",
    "\n",
    "df1 = df.sort_values(by=\"n_cov_snp\", ascending=False).copy()\n",
    "idx = df1.duplicated(subset=\"Master ID\", keep='first')\n",
    "print(f\"Found {np.sum(idx)} Duplicate\")\n",
    "iids = df1[\"iid\"][~idx]\n",
    "idx_unique = df[\"iid\"].isin(iids)\n",
    "\n",
    "df[\"include\"]= False\n",
    "df.loc[idx_unique, \"include\"] = True\n",
    "print(f\"Set {np.sum(idx_unique)} Rows to Include=True\")\n",
    "### Merge in the cluster label\"s\n",
    "#df1 = pd.read_csv(path_meta, sep=\"\\t\")\n",
    "### Only include unique,unrelated samples\n",
    "#df1 = df1[df1[\"include\"]==1].copy().reset_index(drop=True) \n",
    "#df1 = pd.merge(df,df1[[\"iid\", \"label_region\", \"include\"]], on=\"iid\", how=\"left\")\n",
    "#df1.loc[df1[\"label_region\"].isnull(), \"label_region\"]=\"not assigned\"\n",
    "#assert(len(df1)==len(df))\n",
    "\n",
    "##########################################\n",
    "### Prepare dataframe\n",
    "### Set custom Labels - for Punic analysis\n",
    "df_context = pd.read_csv(\"./output/tables/reference_samples_plot.v49.2.tsv\", sep=\"\\t\")\n",
    "print(f\"Loaded {len(df_context)} Individuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 7 Individuals clst: Lilybaeum_Early_Punic\n",
      "Updated 4 Individuals clst: Motya_Early_Punic\n",
      "Updated 5 Individuals clst: Tharros_Early_Punic\n",
      "Updated 3 Individuals clst: Punic_African\n",
      "Updated 9 Individuals clst: Punic_African_Cline\n",
      "Updated 2 Individuals clst: Tunisia_Mesolithic\n"
     ]
    }
   ],
   "source": [
    "df1 = df.copy()   # Copy for adding edits while keeping the original\n",
    "\n",
    "### Set the Akzhiv Indivdiuals\n",
    "idx = df[\"loc\"].isnull()\n",
    "df[idx] =\"not assigned\"\n",
    "idx = df[\"loc\"].str.contains(\"Akhziv\")\n",
    "df1.loc[idx, \"clst\"] = \"Israel_Phoenician\"\n",
    "\n",
    "### Set the Core Punic Individuals\n",
    "### Old per site\n",
    "\n",
    "iids_lil_early = [\"I12666\", \"I12847\", \"I24678\", \"I24676\", \"I24675\", \"I24556\", \"I22095\"] ### Lilybaeum Early\n",
    "set_clst(df1, idx_list=iids_lil_early, clst_new=\"Lilybaeum_Early_Punic\")\n",
    "\n",
    "iids_mot_early = [\"I4798\", \"I4799\", \"I4800\", \"I22236\"] ### Motya Early # 2 Outliers \"I7762\", \"I22232\"\n",
    "set_clst(df1, idx_list=iids_mot_early, clst_new=\"Motya_Early_Punic\")\n",
    "\n",
    "iids_thar_early = [\"I22115\", \"I22121\", \"I22096\", \"I22118\", \"I22117\"] ### Tharros Early I22122 offspring\n",
    "set_clst(df1, idx_list=iids_thar_early, clst_new=\"Tharros_Early_Punic\")\n",
    "\n",
    "### Set Africans (High on PCA)\n",
    "iids_afr_punic =  [\"I18193\", \"I18189\", \"I22093\"]  # \"I22113\" high but not high enough\n",
    "set_clst(df1, idx_list=iids_afr_punic, clst_new=\"Punic_African\")\n",
    "\n",
    "### African Cline (On PCA cline)\n",
    "iids_afr_cline = [\"I21966\", \"I21984\", \"I22094\", \"I22090\", \"VIL011\", \"VIL006\", \"VIL009\", \"VIL010\", \"VIL007\"] # but not VIL004\n",
    "set_clst(df1, idx_list=iids_afr_cline, clst_new=\"Punic_African_Cline\")\n",
    "\n",
    "\n",
    "iids_tunisia_m = [\"I20824\", \"I20825\"]\n",
    "set_clst(df1, idx_list=iids_tunisia_m, clst_new=\"Tunisia_Mesolithic\")\n",
    "\n",
    "### Drop Indivdiuals\n",
    "iids_na = [\"I22257\", \"TAF014\", \"TAF011\", \"I13394\"]  # The African Outlier in the Phoenicians / 2 Related Taforalt /  1 related Sicily_IA/ 2 relatedI22236\n",
    "for iid in iids_na:\n",
    "    idx = df[\"iid\"]==iid\n",
    "    assert(np.sum(idx)>0)\n",
    "    df1.loc[idx, \"clst\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>Master ID</th>\n",
       "      <th>loc</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>age</th>\n",
       "      <th>age_range</th>\n",
       "      <th>region</th>\n",
       "      <th>study</th>\n",
       "      <th>clst</th>\n",
       "      <th>mean_cov</th>\n",
       "      <th>n_cov_snp</th>\n",
       "      <th>avg_cov_snp</th>\n",
       "      <th>include_alt</th>\n",
       "      <th>family</th>\n",
       "      <th>sex</th>\n",
       "      <th>contact</th>\n",
       "      <th>include</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14430</th>\n",
       "      <td>I22866</td>\n",
       "      <td>I22866</td>\n",
       "      <td>Dukanet el Ketif</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6727</td>\n",
       "      <td>4878-4712 calBCE (5910±30 BP, PSUAMS-9397)</td>\n",
       "      <td>Tunisia</td>\n",
       "      <td>Unpublished / Unmarked as relevant to a study</td>\n",
       "      <td>Tunisia_LN</td>\n",
       "      <td>0.702394</td>\n",
       "      <td>842873</td>\n",
       "      <td>3.079</td>\n",
       "      <td>True</td>\n",
       "      <td>n/a (no relatives detected)</td>\n",
       "      <td>F</td>\n",
       "      <td>Pinhasi, Ron</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18186</th>\n",
       "      <td>I22852</td>\n",
       "      <td>I22852</td>\n",
       "      <td>Hergla</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5864</td>\n",
       "      <td>4035-3804 calBCE (5130±25 BP, PSUAMS-9395)</td>\n",
       "      <td>Tunisia</td>\n",
       "      <td>Unpublished / Unmarked as relevant to a study</td>\n",
       "      <td>Tunisia_LN</td>\n",
       "      <td>0.474592</td>\n",
       "      <td>569510</td>\n",
       "      <td>0.819</td>\n",
       "      <td>True</td>\n",
       "      <td>n/a (no relatives detected)</td>\n",
       "      <td>M</td>\n",
       "      <td>Pinhasi, Ron</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          iid Master ID               loc  lat  lon   age  \\\n",
       "14430  I22866    I22866  Dukanet el Ketif  NaN  NaN  6727   \n",
       "18186  I22852    I22852            Hergla  NaN  NaN  5864   \n",
       "\n",
       "                                        age_range   region  \\\n",
       "14430  4878-4712 calBCE (5910±30 BP, PSUAMS-9397)  Tunisia   \n",
       "18186  4035-3804 calBCE (5130±25 BP, PSUAMS-9395)  Tunisia   \n",
       "\n",
       "                                               study        clst  mean_cov  \\\n",
       "14430  Unpublished / Unmarked as relevant to a study  Tunisia_LN  0.702394   \n",
       "18186  Unpublished / Unmarked as relevant to a study  Tunisia_LN  0.474592   \n",
       "\n",
       "      n_cov_snp avg_cov_snp include_alt                       family sex  \\\n",
       "14430    842873       3.079        True  n/a (no relatives detected)   F   \n",
       "18186    569510       0.819        True  n/a (no relatives detected)   M   \n",
       "\n",
       "            contact include  \n",
       "14430  Pinhasi, Ron    True  \n",
       "18186  Pinhasi, Ron    True  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[df1[\"clst\"].str.contains(\"Tunisia_LN\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate F_ST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example goto Area 51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Pipeline of Calculating all F_ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 4, 5, 4, 3, 3, 3, 9, 15, 35, 20, 18, 13, 15, 11, 5, 4, 2, 7, 3, 2, 2]\n",
      "Calculating AF 22 Populations\n",
      "Calculating counts pop: Lilybaeum_Early_Punic, n=7...\n",
      "Calculating counts pop: Motya_Early_Punic, n=4...\n",
      "Calculating counts pop: Tharros_Early_Punic, n=5...\n",
      "Calculating counts pop: Morocco_Iberomaurusian, n=4...\n",
      "Calculating counts pop: Morocco_EN.SG, n=3...\n",
      "Calculating counts pop: Morocco_LN.SG, n=3...\n",
      "Calculating counts pop: Punic_African, n=3...\n",
      "Calculating counts pop: Punic_African_Cline, n=9...\n",
      "Calculating counts pop: Israel_Phoenician, n=15...\n",
      "Calculating counts pop: Israel_MLBA, n=35...\n",
      "Calculating counts pop: Spain_IA, n=20...\n",
      "Calculating counts pop: Italy_Sicily_IA_Polizzello, n=18...\n",
      "Calculating counts pop: Italy_Sardinia_EBA, n=13...\n",
      "Calculating counts pop: Greece_BA_Mycenaean, n=15...\n",
      "Calculating counts pop: Italy_Sardinia_BA_Nuragic, n=11...\n",
      "Calculating counts pop: CanaryIslands_Guanche.SG, n=5...\n",
      "Calculating counts pop: Tunisia_N, n=4...\n",
      "Calculating counts pop: Tunisia_Mesolithic, n=2...\n",
      "Calculating counts pop: Italy_Sicily_MBA, n=7...\n",
      "Calculating counts pop: Spain_Menorca_LBA, n=3...\n",
      "Calculating counts pop: Tunisia_LN, n=2...\n",
      "Calculating counts pop: Egypt_ThirdIntermediatePeriod, n=2...\n",
      "Finished pre-processing all freqs. Starting Allele Count calculation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/groups/reich/hringbauer/explore_ntbk/jptvenv37/lib/python3.7/site-packages/allel/stats/admixture.py:41: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x = (ac[:, 0] * ac[:, 1]) / (an * (an - 1))\n",
      "/n/groups/reich/hringbauer/explore_ntbk/jptvenv37/lib/python3.7/site-packages/allel/stats/fst.py:802: RuntimeWarning: invalid value encountered in true_divide\n",
      "  vb = num_bsum / den_bsum\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Fst Matrix Calculation!\n",
      "CPU times: user 6min 53s, sys: 19.9 s, total: 7min 13s\n",
      "Wall time: 15min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pops = [\"Lilybaeum_Early_Punic\", \"Motya_Early_Punic\", \"Tharros_Early_Punic\",\n",
    "        \"Morocco_Iberomaurusian\", \"Morocco_EN.SG\", \"Morocco_LN.SG\", \"Punic_African\", \"Punic_African_Cline\", \n",
    "        \"Israel_Phoenician\", \"Israel_MLBA\", \"Spain_IA\", \"Italy_Sicily_IA_Polizzello\",\n",
    "        \"Italy_Sardinia_EBA\", \"Greece_BA_Mycenaean\", \"Italy_Sardinia_BA_Nuragic\",\n",
    "        \"CanaryIslands_Guanche.SG\", \"Tunisia_N\", \"Tunisia_Mesolithic\", \"Italy_Sicily_MBA\", \"Spain_Menorca_LBA\",\n",
    "        \"Tunisia_LN\", \"Egypt_ThirdIntermediatePeriod\"]\n",
    "col = \"clst\"\n",
    "ph = True\n",
    "\n",
    "##########################################\n",
    "### Calculating Allele Frequencies\n",
    "ns = [len(get_cluster_idx(df=df1, clst=p, \n",
    "                           exact=True, col_clst=col)) for p in pops]\n",
    "print(ns)\n",
    "assert(np.min(ns)>1) # To have enough data for F_st\n",
    "\n",
    "print(f\"Calculating AF {len(pops)} Populations\")\n",
    "acs = calculate_ac_pops(pops=pops, f=f, df=df1, col=col, \n",
    "                        exact=True, out=True, ph=ph)    \n",
    "print(\"Finished pre-processing all freqs. Starting Allele Count calculation!\")\n",
    "\n",
    "##########################################\n",
    "### Calculate Fst from the list of allele counts for all pairs\n",
    "res, res_h = [], []  # Vector for the Results\n",
    "pops1, pops2 = [], [] # Vector for the populations pairs\n",
    "\n",
    "k = len(pops)\n",
    "for i1, i2 in it.combinations(np.arange(k),2):\n",
    "    # Save for the population list\n",
    "    pops1.append(pops[i1])\n",
    "    pops2.append(pops[i2])    \n",
    "    ### Do the fst Calculation\n",
    "    res_new = fst_ac(acs[i1], acs[i2])\n",
    "    res.append(res_new)\n",
    "    \n",
    "print(\"\\nFinished Fst Matrix Calculation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 231 pw. Comparisons.\n",
      "Saved 22 populations.\n"
     ]
    }
   ],
   "source": [
    "ns=[]\n",
    "for pop in pops:\n",
    "    idcs = get_cluster_idx(df=df1, clst=pop, \n",
    "                           exact=True, col_clst=col)\n",
    "    ns.append(len(idcs))\n",
    "    \n",
    "res = np.array(res) # For better slicing \n",
    "### Make F_ST Data Frame\n",
    "df_fst = pd.DataFrame({\"s1\": pops1, \"s2\": pops2,\n",
    "               \"fst\": res[:,0], \"se\": res[:,1], \"z\": res[:,2]})\n",
    "\n",
    "### Save the F_ST-Matrix as well as the populaition:\n",
    "folder = \"./output/tables/fst/v49.2/\"  # Where to save: fst-mat-hudson\n",
    "\n",
    "df_fst.to_csv(folder + 'fst.csv', index=False) # Save the Values\n",
    "print(f\"Saved {len(df_fst)} pw. Comparisons.\")\n",
    "\n",
    "### Do the pop size numbers\n",
    "df_pops = pd.DataFrame({\"pop\": pops, \"n\": ns})\n",
    "df_pops.to_csv(folder + 'pops.csv', index=False) # Save the Values\n",
    "print(f\"Saved {len(df_pops)} populations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Israel_Phoenician',\n",
       " 'Israel_MLBA',\n",
       " 'Spain_IA',\n",
       " 'Italy_Sicily_IA_Polizzello',\n",
       " 'Italy_Sardinia_EBA',\n",
       " 'Greece_BA_Mycenaean',\n",
       " 'Italy_Sardinia_BA_Nuragic',\n",
       " 'CanaryIslands_Guanche.SG',\n",
       " 'Tunisia_N',\n",
       " 'Italy_Sicily_MBA',\n",
       " 'Spain_Menorca_LBA',\n",
       " 'Tunisia_LN']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Calculate the outgroup f3\n",
    "\n",
    "%%time\n",
    "pops = [\"CanaryIslands_Guanche.SG\", \"Israel_Phoenician\", \n",
    "        \"Italy_Sardinia_BA_Nuragic\"]\n",
    "\n",
    "acs = calculate_ac_pops(pops=pops, \n",
    "                        f=f, df=df1, col=\"clst\", \n",
    "                        exact=False, out=True, ph=True)\n",
    "\n",
    "### Calculate the outgroup f3\n",
    "#fst_ac(acs[2], acs[3])\n",
    "\n",
    "fst_ac(acs[1], acs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calldata', 'samples', 'variants']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to get group info (addr overflow, addr = 1166280, size = 328, eoa = 2048)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7811/3807227937.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_h5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"calldata\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"samples\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/n/groups/reich/hringbauer/explore_ntbk/jptvenv37/lib/python3.7/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;34m\"\"\" Number of members attached to this group \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwith_phil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5g.pyx\u001b[0m in \u001b[0;36mh5py.h5g.GroupID.get_num_objs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to get group info (addr overflow, addr = 1166280, size = 328, eoa = 2048)"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_h5, \"r\") as f:\n",
    "    print(list(f))\n",
    "    print(list(f[\"calldata\"]))\n",
    "    s = f[\"samples\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
